<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>改善深层神经网络——深度学习的实用层面 | Life Designer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="深度学习的实用层面训练/验证/测试集对于一个需要解决的问题的样本数据，在建立模型的过程中，我们会将问题的data划分为以下几个部分：  训练集（train set）：用训练集对算法或模型进行训练过程； 验证集（development set）：利用验证集或者又称为简单交叉验证集（hold-out cross validation set）进行交叉验证，选择出最好的模型； 测试集（test set）">
<meta name="keywords" content="deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="改善深层神经网络——深度学习的实用层面">
<meta property="og:url" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/index.html">
<meta property="og:site_name" content="Life Designer">
<meta property="og:description" content="深度学习的实用层面训练/验证/测试集对于一个需要解决的问题的样本数据，在建立模型的过程中，我们会将问题的data划分为以下几个部分：  训练集（train set）：用训练集对算法或模型进行训练过程； 验证集（development set）：利用验证集或者又称为简单交叉验证集（hold-out cross validation set）进行交叉验证，选择出最好的模型； 测试集（test set）">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/18.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/1.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/2.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/3.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/4.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/5.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/6.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/7.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/8.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/9.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/10.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/11.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/12.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/13.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/14.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/15.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/16.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/17.jpg">
<meta property="og:updated_time" content="2018-01-14T03:22:30.718Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="改善深层神经网络——深度学习的实用层面">
<meta name="twitter:description" content="深度学习的实用层面训练/验证/测试集对于一个需要解决的问题的样本数据，在建立模型的过程中，我们会将问题的data划分为以下几个部分：  训练集（train set）：用训练集对算法或模型进行训练过程； 验证集（development set）：利用验证集或者又称为简单交叉验证集（hold-out cross validation set）进行交叉验证，选择出最好的模型； 测试集（test set）">
<meta name="twitter:image" content="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/18.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Life Designer" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yespon.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/categories">Category</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Life Designer</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Life Designer, design by oneself!</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-改善深层神经网络——深度学习的实践方面" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/12/改善深层神经网络——深度学习的实践方面/" class="article-date">
  <time datetime="2018-01-12T12:09:15.000Z" itemprop="datePublished">2018-01-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      改善深层神经网络——深度学习的实用层面
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="深度学习的实用层面"><a href="#深度学习的实用层面" class="headerlink" title="深度学习的实用层面"></a>深度学习的实用层面</h1><h2 id="训练-验证-测试集"><a href="#训练-验证-测试集" class="headerlink" title="训练/验证/测试集"></a>训练/验证/测试集</h2><p>对于一个需要解决的问题的样本数据，在建立模型的过程中，我们会将问题的data划分为以下几个部分：</p>
<ol>
<li><strong>训练集（train set）</strong>：用训练集对算法或模型进行训练过程；</li>
<li><strong>验证集（development set）</strong>：利用验证集或者又称为简单交叉验证集（hold-out cross validation set）进行交叉验证，选择出最好的模型；</li>
<li><strong>测试集（test set）</strong>：最后利用测试集对模型进行测试，获取模型运行的无偏估计。</li>
</ol>
<a id="more"></a>
<ul>
<li><p><strong>小数据时代：</strong> 在小数据量的时代，如：100、1000、10000的数据量大小，可以将data做以下划分：</p>
<ul>
<li>无验证集的情况：70% / 30%；</li>
<li>有验证集的情况：60% / 20% / 20%；</li>
<li>通常在小数据量时代，以上比例的划分是非常合理的。</li>
</ul>
</li>
<li><p><strong>大数据时代：</strong> 但是在如今的大数据时代，对于一个问题，我们拥有的data的数量可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。<strong>验证集</strong>的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大能够验证大约2-10种算法哪种更好就足够了，不需要使用20%的数据作为验证集。如百万数据中抽取1万的数据作为验证集就可以了。<strong>测试集</strong>的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中1000条数据足以评估单个模型的效果。</p>
<ul>
<li>100万数据量：98% / 1% / 1%；</li>
<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>
</ul>
</li>
</ul>
<p><strong>Notation：</strong></p>
<ul>
<li>建议验证集和测试集来自于同一个分布，这样可以使得机器学习算法变得更快；</li>
<li>如果不需要用无偏估计来评估模型的性能，则可以不需要测试集。</li>
</ul>
<h2 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差/方差"></a>偏差/方差</h2><p>首先，来看一下偏差和方差的概念及意义：</p>
<ul>
<li><p><strong>偏差：</strong> 描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。相对标准偏差（RSD，relative standard deviation）就是指：标准偏差与计算结果算术平均值的比值。相对标准偏差（RSD）=标准偏差（SD）/计算结果的算术平均值（X）*100%，该值通常用来表示分析测试结果的精密度。</p>
</li>
<li><p><strong>方差：</strong> 描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差是衡量源数据和期望值相差的度量值。方差越大，数据的分布越分散，如下图右列所示。</p>
</li>
</ul>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/18.jpg" alt=""></p>
<p>方差分为离散型方差和连续型方差，在统计学意义上，当数据分布比较分散（即数据在平均数附近波动较大）时，各个数据与平均数的差的平方和较大，方差就较大；当数据分布比较集中时，各个数据与平均数的差的平方和较小。因此方差越大，数据的波动越大；方差越小，数据的波动就越小。<br>对于下图中两个类别分类边界的分割：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/1.jpg" alt=""></p>
<p>从图中我们可以看出，分类器偏差（high bias）较高, 数据欠拟合（underfitting）的；分类器方差（high variance）较高，数据过度拟合（overfitting）。</p>
<p>在 bias-variance tradeoff 的角度来讲，我们利用训练集对模型进行训练就是为了使得模型在train集上使 bias最小化，避免出现 underfitting 的情况；</p>
<p>但是如果模型设置的太复杂，虽然在train集上 bias 的值非常小，模型甚至可以将所有的数据点正确分类，但是当将训练好的模型应用在dev 集上的时候，却出现了较高的错误率。这是因为模型设置的太复杂则没有排除一些train集数据中的噪声，使得模型出现overfitting的情况，在dev 集上出现高variance的现象。</p>
<p>所以对于bias和variance的权衡问题，对于模型来说是一个十分重要的问题。</p>
<p>例子：</p>
<p>几种不同的情况：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/2.jpg" alt=""></p>
<p>以上为在人眼判别误差在0%的情况下，该最优误差通常也称为“贝叶斯误差”。以此最为基准，如果“贝叶斯误差”大约为15%，那么图中第二种情况就是一种比较好的情况。</p>
<p>High bias and high variance的情况</p>
<p>上图中第三种bias和variance的情况出现的可能如下：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/3.jpg" alt=""></p>
<p>没有找到边界线，但却在部分数据点上出现了过拟合，则会导致这种高偏差和高方差的情况。</p>
<p>虽然在这里二维的情况下可能看起来较为奇怪，出现的可能性比较低；但是在高维的情况下，出现这种情况就成为可能。</p>
<h2 id="机器学习的基础"><a href="#机器学习的基础" class="headerlink" title="机器学习的基础"></a>机器学习的基础</h2><p>在训练机器学习模型的过程中，解决High bias 和High variance 的过程：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/4.jpg" alt=""></p>
<ul>
<li><p>是否存在 High bias ?</p>
<ul>
<li>增加网络结构，如增加隐藏层数目；</li>
<li>训练更长时间；</li>
<li>寻找合适的网络架构，使用更大的NN结构；</li>
</ul>
</li>
<li><p>是否存在 High variance？</p>
<ul>
<li>获取更多的数据；</li>
<li>正则化（ regularization ）；</li>
<li>寻找合适的网络结构；</li>
</ul>
</li>
</ul>
<p>在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以使得再不增加另一方的情况下减少一方的值。</p>
<h2 id="正则化（regularization）"><a href="#正则化（regularization）" class="headerlink" title="正则化（regularization）"></a>正则化（regularization）</h2><p>利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度。</p>
<p>Logistic regression</p>
<p>加入正则化项的代价函数：</p>
<p>$$J(w,b)=\dfrac{1}{m}\sum\limits<em>{i=1}^{m}L(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||w||</em>{2}^{2}$$</p>
<p>上式为逻辑回归的L2正则化。</p>
<p>L2正则化： $\dfrac{\lambda}{2m}||w||<em>{2}^{2} = \dfrac{\lambda}{2m}\sum\limits</em>{j=1}^{n<em>{x}} w</em>{j}^{2}=\dfrac{\lambda}{2m}w^{T}w$</p>
<p>L1正则化： $\dfrac{\lambda}{2m}||w||<em>{1}=\dfrac{\lambda}{2m}\sum\limits</em>{j=1}^{n<em>{x}}|w</em>{j}|$<br>其中 $\lambda$ 为正则化因子。</p>
<p>注意：lambda 在python中属于保留字，所以在编程的时候，用“lambd”代表这里的正则化因子 $\lambda$ 。</p>
<h3 id="Neural-network"><a href="#Neural-network" class="headerlink" title="Neural network"></a>Neural network</h3><p>加入正则化项的代价函数：</p>
<p>$$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\dfrac{1}{m}\sum\limits<em>{i=1}^{m}l(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}\sum\limits</em>{l=1}^{L}||w^{[l]}||_{F}^{2}$$</p>
<p>其中 $||w^{[l]}||<em>{F}^{2}=\sum\limits</em>{i=1}^{n^{[l-1]}}\sum\limits<em>{j=1}^{n^{[l]}}(w</em>{ij}^{[l]})^{2}$ ，因为 w 的大小为 $(n^{[l-1]},n^{[l]})$ ，该矩阵范数被称为“Frobenius norm”</p>
<h3 id="Weight-decay"><a href="#Weight-decay" class="headerlink" title="Weight decay"></a>Weight decay</h3><p>在加入正则化项后，梯度变为：</p>
<p>$$dW^{[l]} = (form_backprop)+\dfrac{\lambda}{m}W^{[l]}$$</p>
<p>则梯度更新公式变为：</p>
<p>$$W^{[l]}:= W^{[l]}-\alpha dW^{[l]}$$</p>
<p>代入可得：</p>
<p>$$W^{[l]}:= W^{[l]}-\alpha [ (form_backprop)+\dfrac{\lambda}{m}W^{[l]}]$$<br>$$ = W^{[l]}-\alpha\dfrac{\lambda}{m}W^{[l]} -\alpha(form_backprop)$$<br>$$ = (1-\dfrac{\alpha\lambda}{m})W^{[l]}-\alpha(form_backprop)$$</p>
<p>其中， $(1-\dfrac{\alpha\lambda}{m})$ 为一个 &lt;1 的项，会给原来的 $W^{[l]}$ 一个衰减的参数，所以L2范数正则化也被称为“权重衰减（Weight decay）”。</p>
<h2 id="为什么正则化可以减小过拟合"><a href="#为什么正则化可以减小过拟合" class="headerlink" title="为什么正则化可以减小过拟合"></a>为什么正则化可以减小过拟合</h2><p>假设下图的神经网络结构属于过拟合状态：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/5.jpg" alt=""></p>
<p>对于神经网络的 Cost function：</p>
<p>$$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\dfrac{1}{m}\sum\limits<em>{i=1}^{m}l(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}\sum\limits</em>{l=1}^{L}||w^{[l]}||_{F}^{2}$$</p>
<p>加入正则化项，直观上理解，正则化因子 $\lambda$ 设置的足够大的情况下，为了使代价函数最小化，权重矩阵 W 就会被设置为接近于0的值。则相当于消除了很多神经元的影响，那么图中左上方大的神经网络就会变成一个较小的网络。</p>
<p>当然上面这种解释是一种直观上的理解，但是实际上隐藏层的神经元依然存在，但是他们的影响变小了，便不会导致过拟合。</p>
<p><strong>数学解释：</strong></p>
<p>假设神经元中使用的激活函数为 $g(z)=\tanh(z)$ ，在加入正则化项后：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/6.jpg" alt=""></p>
<p>当 $\lambda$ 增大，导致 $W^{[l]}$ 减小， $Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$ 会相对变小，由上图可知， z 在较小的区域里， $\tanh(z)$ 函数相对呈线性，所以每层的函数就近似线性函数，整个神经网络会计算近似线性的值，从而不会发生过拟合。</p>
<h2 id="Dropout-正则化"><a href="#Dropout-正则化" class="headerlink" title="Dropout 正则化"></a>Dropout 正则化</h2><p>Dropout（随机失活）就是在神经网络的Dropout层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/7.jpg" alt=""></p>
<p>实现Dropout的方法：反向随机失活（Inverted dropout）</p>
<p>首先假设对 layer 3 进行dropout：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">keep_prob = 0.8  # 设置神经元保留概率</div><div class="line">d3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_prob</div><div class="line">a3 = np.multiply(a3, d3)  # a3 *= d3</div><div class="line">a3 /= keep_prob</div></pre></td></tr></table></figure>
<p>这里解释下为什么要有最后一步：$a3 /= keep_prob$</p>
<p>依照例子中的 keep_prob = 0.8 ，那么就有大约20%的神经元被删除了，也就是说 $a^{[3]}$ 中有20%的元素被归零了，在下一层的计算中有 $Z^{[4]}=W^{[4]}\cdot a^{[3]}+b^{[4]}$ ，所以为了不影响 $Z^{[4]}$ 的期望值，所以需要 $W^{[4]}\cdot a^{[3]}$ 的部分除以一个keep_prob。</p>
<p>Inverted dropout 通过对“a3 /= keep_prob”,则保证无论 keep_prob 设置为多少，都不会对 $Z^{[4]}$ 的期望值产生影响。</p>
<p>Notation：在测试阶段不要用dropout，因为那样会使得预测结果变得随机。</p>
<h2 id="理解-Dropout"><a href="#理解-Dropout" class="headerlink" title="理解 Dropout"></a>理解 Dropout</h2><p>这里我们以单个神经元入手，单个神经元的工作就是接收输入，并产生一些有意义的输出，但是加入了Dropout以后，输入的特征都是有可能会被随机清除的，所以该神经元不会再特别依赖于任何一个输入特征，也就是说不会给任何一个输入设置太大的权重。</p>
<p>所以通过传播过程，dropout将产生和L2范数相同的收缩权重的效果。</p>
<p>对于不同的层，设置的keep_prob也可以变化，一般来说，对于可能出现过拟合，且神经元多的层，也就是拥有较大参数集，则会将 keep_prob 设置的相对较小，以便拥有更强大的dropout能力；而对于神经元较少的层，过拟合可能没那么严重，则可设 keep_prob 相对较大，设置 keep_prop = 1 不是代表在该层不使用Dropout，而是保留所有特征。</p>
<p>Dropout 缺点：</p>
<p>dropout的一大缺点就是其使得 Cost function 不能再被明确的定义，因为每次迭代都会随机移除一些结点，如果再想检查梯度下降的性能，将很难进行复查，因为我们定义的代价函数J每次迭代后都会下降，如下：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/8.jpg" alt=""></p>
<p>使用Dropout：</p>
<p>为了绘制如上的图，以便调试，通常关闭dropout功能，即设置 keep_prob = 1.0；<br>运行代码，确保 J(W，b) 函数单调递减；<br>再打开 dropout 。</p>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li>数据扩增（Data augmentation）：通过图片的一些变换，得到更多的训练集和验证集；</li>
</ul>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/9.jpg" alt=""></p>
<ul>
<li>Early stopping：在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决bias和variance之间的最优。</li>
</ul>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/10.jpg" alt=""></p>
<h2 id="归一化输入特征"><a href="#归一化输入特征" class="headerlink" title="归一化输入特征"></a>归一化输入特征</h2><p>对数据集特征 $x<em>{1}$,$x</em>{2}$ 归一化的过程：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/11.jpg" alt=""></p>
<ol>
<li>步骤一：零均值化</li>
</ol>
<p>计算每个特征所有样本数据的均值： $\mu = \dfrac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}$ ；</p>
<p>减去均值得到对称的分布： $x : =x-\mu$ ；</p>
<ol>
<li><p>归一化方差：</p>
<p>$$\sigma^{2} = \dfrac{1}{m}\sum\limits_{i=1}^{m}x^{(i)^{2}}$$ ，</p>
<p>$$x = x/\sigma^{2}$$ 。</p>
</li>
</ol>
<p><strong>使用归一化输入的原因：</strong></p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/12.jpg" alt=""></p>
<p>由图可以看出不使用归一化和使用归一化前后 Cost function 的函数形状会有很大的区别。</p>
<p>在不使用归一化的代价函数中，如果我们设置一个较小的学习率，那么很可能我们需要很多次迭代才能到达代价函数全局最优解；如果使用了归一化，那么无论从哪个位置开始迭代，我们都能以相对很少的迭代次数找到全局最优解。</p>
<p>当输入特征范围差别很大，使用归一化确保特征都在相似范围内通常可以帮助学习算法运行的更快</p>
<h2 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h2><p>如下图所示的神经网络结构，以两个输入为例：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/13.jpg" alt=""></p>
<p>假设 g(z) = z，$b^{[l]}=0$ ，则对于目标输出，有：</p>
<p>$$\hat y = W^{[L]}W^{[L-1]}\cdots W^{[2]}W^{[1]}X$$</p>
<ul>
<li>$W^{[l]}$ 的值大于1的情况：</li>
</ul>
<p>如： $W^{[l]}=\left[ \begin{array}{l}1.5 &amp; 0 \\ 0 &amp; 1.5\end{array} \right]$ ，那么最终， $\hat y = W^{[L]}\left[ \begin{array}{l}1.5 &amp; 0 \\ 0 &amp; 1.5\end{array} \right]^{L-1}X$，激活函数的值将以指数级递增；</p>
<ul>
<li>$W^{[l]}$ 的值小于1的情况：</li>
</ul>
<p>如： $W^{[l]} = \left[ \begin{array}{l}0.5 &amp; 0 \\ 0 &amp; 0.5\end{array} \right]$，那么最终， $\hat y = W^{[L]}\left[ \begin{array}{l}0.5 &amp; 0 \\ 0 &amp; 0.5\end{array} \right]^{L-1}X$ ，激活函数的值将以指数级递减。</p>
<p>类似上面的情况，对于导数也是同样的道理，所以在计算梯度时，根据情况的不同，梯度函数会以指数级递增或者递减，导致训练导数难度上升，梯度下降算法的步长会变得非常非常小，需要训练的时间将会非常长。</p>
<p>在梯度函数上出现的以指数级递增或者递减的情况就分别称为梯度爆炸或者梯度消失。</p>
<h2 id="利用随机初始化缓解梯度消失和爆炸问题"><a href="#利用随机初始化缓解梯度消失和爆炸问题" class="headerlink" title="利用随机初始化缓解梯度消失和爆炸问题"></a>利用随机初始化缓解梯度消失和爆炸问题</h2><p>以一个单个神经元为例子：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/14.jpg" alt=""></p>
<p>由上图可知，当输入的数量 n 较大时，我们希望每个 $w_{i}$ 的值都小一些，这样它们的和得到的 z 也较小。</p>
<p>这里为了得到较小的 $w<em>{i}$ ，设置 $Var(w</em>{i})=\dfrac{1}{n}$ ，这里称为Xavier initialization。</p>
<p>对参数进行初始化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)</div></pre></td></tr></table></figure>
<p>这么做是因为，如果激活函数的输入 x 近似设置成均值为0，标准方差1的情况，输出 z 也会调整到相似的范围内。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>
<p>不同激活函数的 Xavier initialization：</p>
<ul>
<li>激活函数使用Relu： $Var(w_{i})=\dfrac{2}{n}$</li>
<li>激活函数使用tanh： $Var(w_{i})=\dfrac{1}{n}$</li>
</ul>
<p>其中n是输入的神经元个数，也就是 $n^{[l-1]}$ 。</p>
<h2 id="梯度的数值逼近"><a href="#梯度的数值逼近" class="headerlink" title="梯度的数值逼近"></a>梯度的数值逼近</h2><p>使用双边误差的方法去逼近导数：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/15.jpg" alt=""></p>
<p>由图可以看出，双边误差逼近的误差是0.0001，先比单边逼近的误差0.03，其精度要高了很多。</p>
<p>涉及的公式：</p>
<p>双边导数：</p>
<p>$$f’(\theta) = \lim\limits_{\varepsilon \to 0}\dfrac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2\varepsilon}$$</p>
<p>误差： $O(\varepsilon^{2})$</p>
<p>单边导数：</p>
<p>$$f’(\theta) = \lim\limits_{\varepsilon \to 0}\dfrac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$$</p>
<p>误差： $O(\varepsilon)$</p>
<h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>下面用前面一节的方法来进行梯度检验。</p>
<p>连接参数：</p>
<p>因为我们的神经网络中含有大量的参数： $W^{[1]}$, $b^{[1]}$,$\cdots$,$W^{[L]},b^{[L]}$ ，为了做梯度检验，需要将这些参数全部连接起来，reshape成一个大的向量 $\theta$ 。</p>
<p>同时对 $dW^{[1]},db^{[1]},\cdots,dW^{[L]},db^{[L]}$ 执行同样的操作：</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/16.jpg" alt=""></p>
<p>进行梯度检验：</p>
<p>进行如下图的梯度检验</p>
<p><img src="/2018/01/12/改善深层神经网络——深度学习的实践方面/17.jpg" alt=""></p>
<p>判断 $d\theta_{approx}\approx d\theta$ 是否接近。</p>
<p>判断公式：</p>
<p>$$\dfrac {||d\theta<em>{approx}-d\theta||</em>{2}}{||d\theta<em>{approx}||</em>{2}+||d\theta||_{2}}$$</p>
<p>其中，“ $||\cdot ||_{2}$ ”表示欧几里得范数，它是误差平方之和，然后求平方根，得到的欧氏距离。</p>
<h2 id="实现梯度检验-Notes"><a href="#实现梯度检验-Notes" class="headerlink" title="实现梯度检验 Notes"></a>实现梯度检验 Notes</h2><ol>
<li>不要在训练过程中使用梯度检验，只在debug的时候使用，使用完毕关闭梯度检验的功能；</li>
<li>如果算法的梯度检验出现了错误，要检查每一项，找出错误，也就是说要找出哪个$d\theta_{approx}[i]$与$d\theta$的值相差比较大；</li>
<li>不要忘记了正则化项；</li>
<li>梯度检验不能与dropout同时使用。</li>
</ol>
<p>因为每次迭代的过程中，dropout会随机消除隐层单元的不同子集，这时是难以计算dropout在梯度下降上的代价函数J；</p>
<ol>
<li>在随机初始化的时候运行梯度检验，或许在训练几次后再重新运行梯度检验。</li>
</ol>
<p>[1] 吴恩达网络云课堂 deeplearning.ai 课程</p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/29794318" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29794318</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yespon.github.io/2018/01/12/改善深层神经网络——深度学习的实践方面/" data-id="cjfg91ndu002w6owg9rsbc2cq" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>Recommended Posts</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2018/01/13/改善深层神经网络——优化算法/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          改善深层神经网络——优化算法
        
      </div>
    </a>
  
  
    <a href="/2018/01/12/卷积神经网络——深层神经网络/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">卷积神经网络——深层神经网络</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
           <div id="gitment_comments"></div>
    
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">Content</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习的实用层面"><span class="toc-number">1.</span> <span class="toc-text">深度学习的实用层面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#训练-验证-测试集"><span class="toc-number">1.1.</span> <span class="toc-text">训练/验证/测试集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#偏差-方差"><span class="toc-number">1.2.</span> <span class="toc-text">偏差/方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#机器学习的基础"><span class="toc-number">1.3.</span> <span class="toc-text">机器学习的基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#正则化（regularization）"><span class="toc-number">1.4.</span> <span class="toc-text">正则化（regularization）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-network"><span class="toc-number">1.4.1.</span> <span class="toc-text">Neural network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Weight-decay"><span class="toc-number">1.4.2.</span> <span class="toc-text">Weight decay</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么正则化可以减小过拟合"><span class="toc-number">1.5.</span> <span class="toc-text">为什么正则化可以减小过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dropout-正则化"><span class="toc-number">1.6.</span> <span class="toc-text">Dropout 正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解-Dropout"><span class="toc-number">1.7.</span> <span class="toc-text">理解 Dropout</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#其他正则化方法"><span class="toc-number">1.8.</span> <span class="toc-text">其他正则化方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#归一化输入特征"><span class="toc-number">1.9.</span> <span class="toc-text">归一化输入特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度消失与梯度爆炸"><span class="toc-number">1.10.</span> <span class="toc-text">梯度消失与梯度爆炸</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#利用随机初始化缓解梯度消失和爆炸问题"><span class="toc-number">1.11.</span> <span class="toc-text">利用随机初始化缓解梯度消失和爆炸问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度的数值逼近"><span class="toc-number">1.12.</span> <span class="toc-text">梯度的数值逼近</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度检验"><span class="toc-number">1.13.</span> <span class="toc-text">梯度检验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现梯度检验-Notes"><span class="toc-number">1.14.</span> <span class="toc-text">实现梯度检验 Notes</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2018 Yespon&nbsp;|&nbsp;
      Theme by <a href="https://github.com/yespon/hexo-theme-yespon/" target="_blank">Yespon</a>
    </div>
     <div id="footer-right">
      Contact&nbsp;|&nbsp;yespon#qq.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/categories" class="mobile-nav-link">Category</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

  <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
  <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script> 
  <script>
  var gitment = new Gitment({
    // id: '页面 ID', // 可选。默认为 location.href
    owner: 'yespon',
    repo: 'yespon.github.io',
    oauth: {
    client_id: '4ab181ded22ebacbab72',
    client_secret: 'c3cd3df382f34a5685a1608234223423248250f7',
    }
  })
  gitment.render(document.getElementById("gitment_comments"))
</script>


<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>