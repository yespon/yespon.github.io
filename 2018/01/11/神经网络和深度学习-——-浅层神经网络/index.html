<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>神经网络和深度学习 —— 浅层神经网络 | Life Designer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="浅层神经网络神经网络表示一个浅层神经网络示意图：  如图所示，表示一个单隐层的网络结构。 这里主要需要注意的是，层与层之间参数矩阵的规格大小：">
<meta name="keywords" content="deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络和深度学习 —— 浅层神经网络">
<meta property="og:url" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/index.html">
<meta property="og:site_name" content="Life Designer">
<meta property="og:description" content="浅层神经网络神经网络表示一个浅层神经网络示意图：  如图所示，表示一个单隐层的网络结构。 这里主要需要注意的是，层与层之间参数矩阵的规格大小：">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/1.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/2.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/3.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/4.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/5.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/7.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/6.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/8.jpg">
<meta property="og:updated_time" content="2018-01-12T01:22:33.741Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络和深度学习 —— 浅层神经网络">
<meta name="twitter:description" content="浅层神经网络神经网络表示一个浅层神经网络示意图：  如图所示，表示一个单隐层的网络结构。 这里主要需要注意的是，层与层之间参数矩阵的规格大小：">
<meta name="twitter:image" content="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/1.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Life Designer" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yespon.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/categories">Category</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Life Designer</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Life Designer, design by oneself!</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-神经网络和深度学习-——-浅层神经网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/11/神经网络和深度学习-——-浅层神经网络/" class="article-date">
  <time datetime="2018-01-11T13:15:43.000Z" itemprop="datePublished">2018-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      神经网络和深度学习 —— 浅层神经网络
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="浅层神经网络"><a href="#浅层神经网络" class="headerlink" title="浅层神经网络"></a>浅层神经网络</h1><h2 id="神经网络表示"><a href="#神经网络表示" class="headerlink" title="神经网络表示"></a>神经网络表示</h2><p>一个浅层神经网络示意图：</p>
<p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/1.jpg" alt=""></p>
<p>如图所示，表示一个单隐层的网络结构。</p>
<p>这里主要需要注意的是，层与层之间参数矩阵的规格大小：<br><a id="more"></a></p>
<ul>
<li><p>输入层和隐藏层之间</p>
<ul>
<li>$w^{[1]}$-&gt;(4,3)：前面的4是隐层神经元的个数，后面的3是输入层神经元的个数；</li>
<li>$b^{[1]}$-&gt;(4,1)：4表示和隐藏层的神经元个数相同，这个1在之前的视频【python/numpy向量说明一节】中有说明，不建议使用(4,)，这表示秩为1的数组，表示成(4,1)表示定义成列向量更易理解；</li>
</ul>
</li>
<li><p>隐藏层和输出层之间</p>
<ul>
<li>$w^{[1]}$-&gt;(1,4)：前面的1是输出层神经元的个数，后面的4是隐层神经元的个数；</li>
<li>$b^{[1]}$-&gt;(1,1) ：和输出层的神经元个数相同；</li>
</ul>
</li>
</ul>
<p>由上面我们可以总结出，在神经网络中，我们以相邻两层为观测对象，前面一层作为输入，后面一层作为输出，两层之间的w参数矩阵大小为 $(n<em>{out},n</em>{in})$ ，b参数矩阵大小为 $(n_{out},1)$ ，这里是作为 $z = wX+b$ 的线性关系来说明的，在神经网络中， $w^{[i]}=w^{T}$ 。</p>
<p>在logistic regression中，一般我们都会用 $(n<em>{in},n</em>{out})$ 来表示参数大小，计算使用的公式为： $z = w^{T}X+b$ ，注意这两者的区别。</p>
<h2 id="计算神经网络的输出"><a href="#计算神经网络的输出" class="headerlink" title="计算神经网络的输出"></a>计算神经网络的输出</h2><p>除输入层之外每层的计算输出可由下图总结出：</p>
<p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/2.jpg" alt=""></p>
<p>其中，每个结点都对应这两个部分的运算，z运算和a运算。 在编程中，我们使用向量化去计算神经网络的输出：</p>
<p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/3.jpg" alt=""></p>
<p>在对应图中的神经网络结构，我们只用Python代码去实现右边的四个公式即可实现神经网络的输出计算。</p>
<h2 id="向量化实现"><a href="#向量化实现" class="headerlink" title="向量化实现"></a>向量化实现</h2><p>假定在m个训练样本的神经网络中，计算神经网络的输出，用向量化的方法去实现可以避免在程序中使用for循环，提高计算的速度。</p>
<p>下面是实现向量化的解释：</p>
<p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/4.jpg" alt=""></p>
<p>由图可以看出，在m个训练样本中，每次计算都是在重复相同的过程，均得到同样大小和结构的输出，所以利用向量化的思想将单个样本合并到一个矩阵中，其大小为 (x<em>{n},m) ，其中 x</em>{n} 表示每个样本输入网络的神经元个数，也可以认为是单个样本的特征数，m表示训练样本的个数。</p>
<p>通过向量化，可以更加便捷快速地实现神经网络的计算。</p>
<h2 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h2><p>几种不同的激活函数 g(x) ：</p>
<p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/5.jpg" alt=""></p>
<ul>
<li>sigmoid：</li>
</ul>
<p>$$ a = \frac{1}{1+e^{-z}} $$</p>
<p>导数： $a’ = a(1-a)$</p>
<ul>
<li>tanh：</li>
</ul>
<p>$$a=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$</p>
<p>导数： $a’ = 1 - a^{2}$</p>
<ul>
<li>ReLU（修正线性单元）：</li>
</ul>
<p>$$a = \max(0,z)$$</p>
<p>导数：</p>
<p>$$a’ = \begin{cases}1, &amp; \text{if $z$ &gt; 0} \[2ex] 0, &amp; \text{if $z$ &lt; 0} \end{cases}$$</p>
<ul>
<li>Leaky ReLU:</li>
</ul>
<p>$$a = \max(0.01z,z)$$</p>
<p>导数：</p>
<p>$$a’ = \begin{cases}1, &amp; \text{if $z$ &gt; 0} \[2ex] 0.01, &amp; \text{if $z$ &lt; 0} \end{cases}$$</p>
<h3 id="激活函数的选择："><a href="#激活函数的选择：" class="headerlink" title="激活函数的选择："></a>激活函数的选择：</h3><p>sigmoid函数和tanh函数比较：</p>
<ul>
<li>隐藏层：tanh函数的表现要好于sigmoid函数，因为tanh取值范围为 [-1,+1] ，输出分布在0值的附近，均值为0，从隐藏层到输出层数据起到了归一化（均值为0）的效果。</li>
<li>输出层：对于二分类任务的输出取值为 {0,1} ，故一般会选择sigmoid函数。</li>
</ul>
<p>然而sigmoid和tanh函数在当 |z| 很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使 |z| 尽可能的落在0值附近。</p>
<p>ReLU弥补了前两者的缺陷，当 z&gt;0 时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。然而当 z&lt;0 时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。</p>
<p>Leaky ReLU保证在 z&lt;0 的时候，梯度仍然不为0。</p>
<p>在选择激活函数的时候，如果在不知道该选什么的时候就选择ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。</p>
<h3 id="非线性激活函数的必要性"><a href="#非线性激活函数的必要性" class="headerlink" title="非线性激活函数的必要性"></a>非线性激活函数的必要性</h3><p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/7.jpg" alt=""></p>
<p>如图所示，如果令 $a^{[1]}=z^{[1]}$, 则激活函数成为线性激活函数，或者称为“恒等激活函数”，这样会导致这个模型的输出 $ \hat y$ 成为输入特征的线性组合，也就是说神经网络把输入线性组合再输出。</p>
<p>如果去掉激活函数或者使用线性激活函数，那么不管你隐含层有多少层，始终都只是计算线性激活函数，所以不如直接去掉所有隐含层。</p>
<p>只有一个地方可以使用线性激活函数，就是做的是回归问题的输出层，因为输出y 是一个实数，但是隐层依然不能使用线性激活函数。</p>
<h2 id="神经网络的梯度下降法"><a href="#神经网络的梯度下降法" class="headerlink" title="神经网络的梯度下降法"></a>神经网络的梯度下降法</h2><p>以本节中的浅层神经网络为例，我们给出神经网络的梯度下降法的公式。</p>
<ul>
<li>参数： W^{[1]},b^{[1]},W^{[2]},b^{[2]} ；</li>
<li>输入层特征向量个数： n_{x}=n^{[0]} ；</li>
<li>隐藏层神经元个数： n^{[1]} ；</li>
<li>输出层神经元个数： n^{[2]}=1 ；</li>
<li>$W^{[1]}$ 的维度为 $(n^{[1]},n^{[0]}), b^{[1]}$ 的维度为 $(n^{[1]},1)$；</li>
<li>$W^{[2]}$ 的维度为 $(n^{[2]},n^{[1]}), b^{[2]}$ 的维度为 $(n^{[2]},1)$ ；</li>
</ul>
<p>下面为该例子的神经网络反向梯度下降公式（左）和其代码向量化（右）：</p>
<p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/6.jpg" alt=""></p>
<h2 id="直观理解反向传播"><a href="#直观理解反向传播" class="headerlink" title="直观理解反向传播"></a>直观理解反向传播</h2><p><img src="/2018/01/11/神经网络和深度学习-——-浅层神经网络/8.jpg" alt=""></p>
<h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单元仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。</p>
<p>在初始化的时候， W 参数要进行随机初始化， b 则不存在对称性的问题它可以设置为0。 以2个输入，2个隐藏神经元为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W = np.random.rand((2,2))* 0.01</div><div class="line">b = np.zero((2,1))</div></pre></td></tr></table></figure>
<p>这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则 Z = WX+b 所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p>
<p>ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。</p>
<p>[1] 吴恩达网络云课堂 deeplearning.ai 课程<br>[2] <a href="https://zhuanlan.zhihu.com/p/29706138" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29706138</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yespon.github.io/2018/01/11/神经网络和深度学习-——-浅层神经网络/" data-id="cjcb8nhop002o38wgt5a3j4wq" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>Recommended Posts</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2018/01/11/DeepLearning-ai学习笔记/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          DeepLearning.ai学习笔记
        
      </div>
    </a>
  
  
    <a href="/2018/01/09/机器学习固本强基系列之正则化/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习固本强基系列之正则化</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
           <div id="gitment_comments"></div>
    
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">Content</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#浅层神经网络"><span class="toc-number">1.</span> <span class="toc-text">浅层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络表示"><span class="toc-number">1.1.</span> <span class="toc-text">神经网络表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算神经网络的输出"><span class="toc-number">1.2.</span> <span class="toc-text">计算神经网络的输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#向量化实现"><span class="toc-number">1.3.</span> <span class="toc-text">向量化实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#激活函数的选择"><span class="toc-number">1.4.</span> <span class="toc-text">激活函数的选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#激活函数的选择："><span class="toc-number">1.4.1.</span> <span class="toc-text">激活函数的选择：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#非线性激活函数的必要性"><span class="toc-number">1.4.2.</span> <span class="toc-text">非线性激活函数的必要性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的梯度下降法"><span class="toc-number">1.5.</span> <span class="toc-text">神经网络的梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#直观理解反向传播"><span class="toc-number">1.6.</span> <span class="toc-text">直观理解反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机初始化"><span class="toc-number">1.7.</span> <span class="toc-text">随机初始化</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2018 Yespon&nbsp;|&nbsp;
      Theme by <a href="https://github.com/yespon/hexo-theme-yespon/" target="_blank">Yespon</a>
    </div>
     <div id="footer-right">
      Contact&nbsp;|&nbsp;yespon#qq.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/categories" class="mobile-nav-link">Category</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

  <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
  <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script> 
  <script>
  var gitment = new Gitment({
    // id: '页面 ID', // 可选。默认为 location.href
    owner: 'yespon',
    repo: 'yespon.github.io',
    oauth: {
    client_id: '4ab181ded22ebacbab72',
    client_secret: 'c3cd3df382f34a5685a1608234223423248250f7',
    }
  })
  gitment.render(document.getElementById("gitment_comments"))
</script>


<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>