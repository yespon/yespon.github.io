<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>改善深层神经网络——优化算法 | Life Designer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="优化算法Mini-batch 梯度下降法对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。">
<meta name="keywords" content="deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="改善深层神经网络——优化算法">
<meta property="og:url" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/index.html">
<meta property="og:site_name" content="Life Designer">
<meta property="og:description" content="优化算法Mini-batch 梯度下降法对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/1.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/2.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/3.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/4.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/5.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/6.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/7.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/8.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/9.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/10.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/11.jpg">
<meta property="og:updated_time" content="2018-01-14T03:21:36.724Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="改善深层神经网络——优化算法">
<meta name="twitter:description" content="优化算法Mini-batch 梯度下降法对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。">
<meta name="twitter:image" content="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/1.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Life Designer" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yespon.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/categories">Category</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Life Designer</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Life Designer, design by oneself!</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-改善深层神经网络——优化算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/13/改善深层神经网络——优化算法/" class="article-date">
  <time datetime="2018-01-13T12:41:17.000Z" itemprop="datePublished">2018-01-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      改善深层神经网络——优化算法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="Mini-batch-梯度下降法"><a href="#Mini-batch-梯度下降法" class="headerlink" title="Mini-batch 梯度下降法"></a>Mini-batch 梯度下降法</h2><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。<br><a id="more"></a><br>但是如果每次处理训练数据的一部分，即用其子集进行梯度下降，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 Mini-batch。</p>
<h3 id="算法核心"><a href="#算法核心" class="headerlink" title="算法核心"></a>算法核心</h3><p><img src="/2018/01/13/改善深层神经网络——优化算法/1.jpg" alt=""></p>
<p>对于普通的梯度下降法，一个 epoch(一代，遍历一词训练集) 只能进行一次梯度下降；而对于 Mini-batch 梯度下降法，一个 epoch 可以进行 $\frac{m}{Mini-batch}$ 个梯度下降(m为训练集总数)。</p>
<h3 id="不同size大小的比较"><a href="#不同size大小的比较" class="headerlink" title="不同size大小的比较"></a>不同size大小的比较</h3><p>普通的batch梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势，如下图所示：</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/2.jpg" alt=""></p>
<ul>
<li><p>batch梯度下降：</p>
<ul>
<li>对所有m个训练样本($size = m$)执行一次梯度下降，单次迭代耗时太长；</li>
<li>Cost function 总是向减小的方向下降。</li>
</ul>
</li>
<li><p>随机梯度下降：</p>
<ul>
<li>对每一个训练样本($size = 1$)执行一次梯度下降，但是丢失了向量化带来的计算加速；</li>
<li>Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。</li>
</ul>
</li>
<li><p>Mini-batch梯度下降：</p>
<ul>
<li>选择一个 $1&lt;size&lt;m$ 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处；</li>
<li>Cost function的下降处于前两者之间。</li>
</ul>
</li>
</ul>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/3.jpg" alt=""></p>
<h3 id="Mini-batch-大小的选择"><a href="#Mini-batch-大小的选择" class="headerlink" title="Mini-batch 大小的选择"></a>Mini-batch 大小的选择</h3><ul>
<li>如果训练样本的大小比较小时( $m\leqslant 2000$ 时): 选择batch梯度下降法；</li>
<li>如果训练样本的大小比较大时，典型的大小为： $2^{6}$、$2^{7}$、$\cdots$、$2^{10}$ （常用64，128, 256，512）；</li>
<li>Mini-batch的大小要符合CPU/GPU内存。</li>
</ul>
<h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p>指数加权平均的关键函数：</p>
<p>$$v<em>{t} = \beta v</em>{t-1}+(1-\beta)\theta_{t}$$</p>
<p>下图是一个关于天数和温度的散点图：</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/4.jpg" alt=""></p>
<ol>
<li>当 $\beta$ =0.9 时，指数加权平均最后的结果如图中红色线所示；</li>
<li>当 $\beta$ =0.98 时，指数加权平均最后的结果如图中绿色线所示；</li>
<li>当 $\beta$ =0.5 时，指数加权平均最后的结果如下图中黄色线所示；</li>
</ol>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/5.jpg" alt=""></p>
<h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h3><p>例子，当 $\beta$ =0.9 时：</p>
<p>$$v<em>{100} = 0.9v</em>{99}+0.1\theta<em>{100}$$<br>$$v</em>{99} = 0.9v<em>{98}+0.1\theta</em>{99}$$<br>$$v<em>{98} = 0.9v</em>{97}+0.1\theta_{98}$$<br>$$\ldots$$</p>
<p>展开，有：</p>
<p>$$v<em>{100}=0.1\theta</em>{100}+0.9(0.1\theta<em>{99}+0.9(0.1\theta</em>{98}+0.9v<em>{97}))$$<br>$$=0.1\theta</em>{100}+0.1\times0.9\theta<em>{99}+0.1\times(0.9)^{2}\theta</em>{98}+0.1\times(0.9)^{3}\theta_{97}+\cdots$$</p>
<p>上式中所有 $\theta$ 前面的系数相加起来为1或者接近于1, 这叫做指数加权移动平均。</p>
<p>总体来说存在， $(1-\varepsilon)^{1/\varepsilon}=\dfrac{1}{e}$ ，在我们的例子中， $1-\varepsilon=\beta=0.9$ ，即 $0.9^{10}\approx 0.35\approx\dfrac{1}{e}$ 。相当于大约10天后，系数的峰值（这里是0.1）下降到原来的 $\dfrac{1}{e}$ ，只关注了过去10天的天气。</p>
<h3 id="指数加权平均实现"><a href="#指数加权平均实现" class="headerlink" title="指数加权平均实现"></a>指数加权平均实现</h3><p>$$v<em>{0} =0$$<br>$$ v</em>{1}= \beta v<em>{0}+(1-\beta)\theta</em>{1}$$<br>$$ v<em>{2}= \beta v</em>{1}+(1-\beta)\theta<em>{2}$$<br>$$ v</em>{3}= \beta v<em>{2}+(1-\beta)\theta</em>{3}$$<br>$$ \ldots$$</p>
<p>因为，在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，所以在数据量非常大的情况下，指数加权平均在节约计算成本的方面是一种非常有效的方式，可以很大程度上减少计算机资源存储和内存的占用。</p>
<h3 id="指数加权平均的偏差修正"><a href="#指数加权平均的偏差修正" class="headerlink" title="指数加权平均的偏差修正"></a>指数加权平均的偏差修正</h3><p>在我们执行指数加权平均的公式时，当 $\beta$=0.98 时，我们得到的并不是图中的绿色曲线，而是下图中的紫色曲线，其起点比较低。</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/6.jpg" alt=""></p>
<ul>
<li>原因：</li>
</ul>
<p>$$v<em>{0}=0$$<br>$$v</em>{1}=0.98v<em>{0}+0.02\theta</em>{1}=0.02\theta<em>{1}$$<br>$$v</em>{2}=0.98v<em>{1}+0.02\theta</em>{2}<br>=0.98\times0.02\theta<em>{1}+0.02\theta</em>{2}<br>=0.0196\theta<em>{1}+0.02\theta</em>{2}$$</p>
<p>如果第一天的值为如 40 ，则 $v_{1}=0.02\times40=8$ ，得到的值要远小于实际值，后面几天的情况也会由于初值引起的影响，均低于实际均值。</p>
<ul>
<li>偏差修正：</li>
</ul>
<p>不直接使用 $v<em>{t}$，而是使用 $\frac{v</em>{t}}{1- \beta^{t}}$</p>
<p>使用 $v_{1}=0.02\times40=8$</p>
<p>当 t=2 时：</p>
<p> $$1-\beta^{t}=1-(0.98)^{2}=0.0396$$</p>
<p> $$\dfrac{v<em>{2}}{0.0396}=\dfrac{0.0196\theta</em>{1}+0.02\theta_{2}}{0.0396}$$</p>
<p>偏差修正得到了绿色的曲线，在开始的时候，能够得到比紫色曲线更好的计算平均的效果。随着 t 逐渐增大， $\beta^{t}$ 接近于0，所以后面绿色的曲线和紫色的曲线逐渐重合了。</p>
<p>虽然存在这种问题，但是在实际过程中，一般会忽略前期均值偏差的影响。</p>
<h2 id="动量（Momentum）梯度下降法"><a href="#动量（Momentum）梯度下降法" class="headerlink" title="动量（Momentum）梯度下降法"></a>动量（Momentum）梯度下降法</h2><p>动量梯度下降的基本思想就是计算梯度的指数加权平均数，并利用该梯度来更新权重。</p>
<p>在我们优化 Cost function 的时候，以下图所示的函数图为例：</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/7.jpg" alt=""></p>
<p>在利用梯度下降法来最小化该函数的时候，每一次迭代所更新的代价函数值如图中蓝色线所示在上下波动，而这种幅度比较大波动，减缓了梯度下降的速度，而且我们只能使用一个较小的学习率来进行迭代。</p>
<p>如果用较大的学习率，结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率。</p>
<p>但是我们又希望在如图的纵轴方向梯度下降的缓慢一些，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点，而这里用动量梯度下降法既可以实现，如红色线所示。</p>
<h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><p><img src="/2018/01/13/改善深层神经网络——优化算法/8.jpg" alt=""></p>
<p>其中，$\alpha, \beta$ 为超参数，且 $\beta$ 常用的值是0.9。</p>
<p>在我们进行动量梯度下降算法的时候，由于使用了指数加权平均的方法。原来在纵轴方向上的上下波动，经过平均以后，接近于0，纵轴上的波动变得非常的小；但在横轴方向上，所有的微分都指向横轴方向，因此其平均值仍然很大。最终实现红色线所示的梯度下降曲线。</p>
<h3 id="算法本质解释"><a href="#算法本质解释" class="headerlink" title="算法本质解释"></a>算法本质解释</h3><p>在对应上面的计算公式中，将Cost function想象为一个碗状，想象从顶部往下滚球，其中：</p>
<ul>
<li>微分项 dw,db 想象为球提供的加速度；</li>
<li>动量项 $v<em>{dw}$，$v</em>{db}$ 相当于速度；</li>
</ul>
<p>小球在向下滚动的过程中，因为加速度的存在使得速度会变快，但是由于 $\beta$ 的存在，其值小于1，可以认为是摩擦力，所以球不会无限加速下去。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>除了上面所说的 Momentum 梯度下降法，RMSprop（root mean square prop，均方根）也是一种可以加快梯度下降的算法（两者都可以消除梯度下降过程中的摆动）。</p>
<p>同样算法的样例实现如下图所示：</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/9.jpg" alt=""></p>
<p>这里假设参数b的梯度处于纵轴方向，参数 w 的梯度处于横轴方向（当然实际中是处于高维度的情况），利用 RMSprop 算法，可以减小某些维度梯度更新波动较大的情况，如图中蓝色线所示，使其梯度下降的速度变得更快，如图绿色线所示。</p>
<p>在如图所示的实现中，RMSprop 将微分项进行平方，然后使用平方根进行梯度更新，同时为了确保算法不会除以 0，平方根分母中在实际使用会加入一个很小的值如 $\varepsilon=10^{-8}$ 。</p>
<h2 id="Adam-优化算法"><a href="#Adam-优化算法" class="headerlink" title="Adam 优化算法"></a>Adam 优化算法</h2><p>Adam （Adaptive Moment Estimation）优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法。</p>
<h3 id="算法实现-1"><a href="#算法实现-1" class="headerlink" title="算法实现"></a>算法实现</h3><ul>
<li>初始化： V<em>{dw} = 0，S</em>{dw}=0，V<em>{db}=0，S</em>{db} = 0</li>
</ul>
<hr>
<ul>
<li>第 t 次迭代：<ul>
<li>Compute  dw，db on the current mini-batch</li>
<li>$V<em>{dw} = \beta</em>{1}V<em>{dw}+(1-\beta</em>{1})dw, V<em>{db} = \beta</em>{1}V<em>{db}+(1-\beta</em>{1})db$ &lt;— “Momentum”</li>
<li>$S<em>{dw}=\beta</em>{2}S<em>{dw}+(1-\beta</em>{2})(dw)^{2}, S<em>{db}=\beta</em>{2}S<em>{db}+(1-\beta</em>{2})(db)^{2}$ &lt;— “RMSprop”</li>
<li>$V<em>{dw}^{corrected} = V</em>{dw}/(1-\beta<em>{1}^{t}),V</em>{db}^{corrected} = V<em>{db}/(1-\beta</em>{1}^{t})$ &lt;— 偏差修正</li>
<li>$S<em>{dw}^{corrected} = S</em>{dw}/(1-\beta<em>{2}^{t}), S</em>{db}^{corrected} = S<em>{db}/(1-\beta</em>{2}^{t})$ &lt;— 偏差修正</li>
<li>$w:=w-\alpha\dfrac{V<em>{dw}^{corrected}}{\sqrt{S</em>{dw}^{corrected}}+\varepsilon}, b:=b-\alpha\dfrac{V<em>{db}^{corrected}}{\sqrt{S</em>{db}^{corrected}}+\varepsilon}$</li>
</ul>
</li>
</ul>
<hr>
<h3 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h3><ul>
<li>$\alpha$ ：需要进行调试；</li>
<li>$\beta_{1}$ ：常用缺省值为0.9， dw 的加权平均；</li>
<li>$\beta_{2}$ ：推荐使用0.999， $dw^{2}$ 的加权平均值；</li>
<li>$\varepsilon$ ：推荐使用 $10^{-8}$ 。</li>
</ul>
<h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>在我们利用 mini-batch 梯度下降法来寻找Cost function的最小值的时候，如果我们设置一个固定的学习速率 $\alpha$ ，则算法在到达最小值点附近后，由于不同batch中存在一定的噪声，使得不会精确收敛，而一直会在一个最小值点较大的范围内波动，如下图中蓝色线所示。</p>
<p>但是如果我们使用学习率衰减，逐渐减小学习速率 $\alpha$ ，在算法开始的时候，学习速率还是相对较快，能够相对快速的向最小值点的方向下降。但随着 $\alpha$ 的减小，下降的步伐也会逐渐变小，最终会在最小值附近的一块更小的区域里波动，如图中绿色线所示。</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/10.jpg" alt=""></p>
<h3 id="学习率衰减的实现"><a href="#学习率衰减的实现" class="headerlink" title="学习率衰减的实现"></a>学习率衰减的实现</h3><ul>
<li>常用： $\alpha = \dfrac{1}{1+decay_rate*epoch_num}\alpha_{0}$</li>
<li>指数衰减： $\alpha = 0.95^{epoch_num}\alpha_{0}$</li>
<li>其他： $\alpha = \dfrac{k}{epoch_num}\cdot\alpha_{0}$</li>
<li>离散下降（不同阶段使用不同的学习速率）</li>
</ul>
<h2 id="局部最优问题"><a href="#局部最优问题" class="headerlink" title="局部最优问题"></a>局部最优问题</h2><p>在低维度的情形下，我们可能会想象到一个Cost function 如左图所示，存在一些局部最小值点，在初始化参数的时候，如果初始值选取的不得当，会存在陷入局部最优点的可能性。</p>
<p>但是，如果我们建立一个高维度的神经网络。通常梯度为零的点，并不是如左图中的局部最优点，而是右图中的<strong>鞍点</strong>（叫鞍点是因为其形状像马鞍的形状）。</p>
<p><img src="/2018/01/13/改善深层神经网络——优化算法/11.jpg" alt=""></p>
<p>在一个具有高维度空间的函数中，如果梯度为0，那么在每个方向，Cost function可能是凸函数，也有可能是凹函数。但如果参数维度为2万维，想要得到局部最优解，那么所有维度均需要是凹函数，其概率为 $2^{-20000}$ ，可能性非常的小。也就是说，在低维度中的局部最优点的情况，并不适用于高维度，在梯度为0的点更有可能是鞍点，而不是局部最小值点。</p>
<p>在高纬度的情况下：</p>
<ul>
<li>几乎不可能陷入局部最小值点；</li>
<li>处于鞍点的停滞区会减缓学习过程，利用如Adam等算法进行改善。</li>
</ul>
<p>[1] 吴恩达网络云课堂 deeplearning.ai 课程</p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/30034654" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/30034654</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yespon.github.io/2018/01/13/改善深层神经网络——优化算法/" data-id="cjep9ugv5002fb8wgvwhsztwk" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>Recommended Posts</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          改善深层神经网络——超参数调试、Batch正则化和程序框架
        
      </div>
    </a>
  
  
    <a href="/2018/01/12/改善深层神经网络——深度学习的实践方面/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">改善深层神经网络——深度学习的实用层面</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
           <div id="gitment_comments"></div>
    
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">Content</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#优化算法"><span class="toc-number">1.</span> <span class="toc-text">优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mini-batch-梯度下降法"><span class="toc-number">1.1.</span> <span class="toc-text">Mini-batch 梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法核心"><span class="toc-number">1.1.1.</span> <span class="toc-text">算法核心</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#不同size大小的比较"><span class="toc-number">1.1.2.</span> <span class="toc-text">不同size大小的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch-大小的选择"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mini-batch 大小的选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#指数加权平均"><span class="toc-number">1.2.</span> <span class="toc-text">指数加权平均</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解指数加权平均"><span class="toc-number">1.2.1.</span> <span class="toc-text">理解指数加权平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#指数加权平均实现"><span class="toc-number">1.2.2.</span> <span class="toc-text">指数加权平均实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#指数加权平均的偏差修正"><span class="toc-number">1.2.3.</span> <span class="toc-text">指数加权平均的偏差修正</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#动量（Momentum）梯度下降法"><span class="toc-number">1.3.</span> <span class="toc-text">动量（Momentum）梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法实现"><span class="toc-number">1.3.1.</span> <span class="toc-text">算法实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法本质解释"><span class="toc-number">1.3.2.</span> <span class="toc-text">算法本质解释</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSprop"><span class="toc-number">1.4.</span> <span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam-优化算法"><span class="toc-number">1.5.</span> <span class="toc-text">Adam 优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法实现-1"><span class="toc-number">1.5.1.</span> <span class="toc-text">算法实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#超参数的选择"><span class="toc-number">1.5.2.</span> <span class="toc-text">超参数的选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习率衰减"><span class="toc-number">1.6.</span> <span class="toc-text">学习率衰减</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#学习率衰减的实现"><span class="toc-number">1.6.1.</span> <span class="toc-text">学习率衰减的实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#局部最优问题"><span class="toc-number">1.7.</span> <span class="toc-text">局部最优问题</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2018 Yespon&nbsp;|&nbsp;
      Theme by <a href="https://github.com/yespon/hexo-theme-yespon/" target="_blank">Yespon</a>
    </div>
     <div id="footer-right">
      Contact&nbsp;|&nbsp;yespon#qq.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/categories" class="mobile-nav-link">Category</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

  <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
  <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script> 
  <script>
  var gitment = new Gitment({
    // id: '页面 ID', // 可选。默认为 location.href
    owner: 'yespon',
    repo: 'yespon.github.io',
    oauth: {
    client_id: '4ab181ded22ebacbab72',
    client_secret: 'c3cd3df382f34a5685a1608234223423248250f7',
    }
  })
  gitment.render(document.getElementById("gitment_comments"))
</script>


<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>