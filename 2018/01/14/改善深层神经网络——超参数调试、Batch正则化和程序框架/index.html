<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>改善深层神经网络——超参数调试、Batch正则化和程序框架 | Life Designer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="超参数调试、Batch正则化和程序框架超参数的调试处理在机器学习领域，超参数比较少，我们之前利用设置网格点的方式来调试超参数；但在深度学习领域，超参数较多，不再是设置规则的网格点，而是随机选择点进行调试。这样做是因为，在我们处理问题的时候，无法知道哪个超参数更为重要，随机的方式测试超参数点的性能更为合理，可以探究超参数的潜在价值。">
<meta name="keywords" content="deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="改善深层神经网络——超参数调试、Batch正则化和程序框架">
<meta property="og:url" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/index.html">
<meta property="og:site_name" content="Life Designer">
<meta property="og:description" content="超参数调试、Batch正则化和程序框架超参数的调试处理在机器学习领域，超参数比较少，我们之前利用设置网格点的方式来调试超参数；但在深度学习领域，超参数较多，不再是设置规则的网格点，而是随机选择点进行调试。这样做是因为，在我们处理问题的时候，无法知道哪个超参数更为重要，随机的方式测试超参数点的性能更为合理，可以探究超参数的潜在价值。">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/1.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/2.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/3.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/4.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/5.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/6.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/7.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/8.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/9.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/10.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/11.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/12.jpg">
<meta property="og:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/13.jpg">
<meta property="og:updated_time" content="2018-01-14T14:36:50.517Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="改善深层神经网络——超参数调试、Batch正则化和程序框架">
<meta name="twitter:description" content="超参数调试、Batch正则化和程序框架超参数的调试处理在机器学习领域，超参数比较少，我们之前利用设置网格点的方式来调试超参数；但在深度学习领域，超参数较多，不再是设置规则的网格点，而是随机选择点进行调试。这样做是因为，在我们处理问题的时候，无法知道哪个超参数更为重要，随机的方式测试超参数点的性能更为合理，可以探究超参数的潜在价值。">
<meta name="twitter:image" content="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/1.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Life Designer" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yespon.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/categories">Category</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Life Designer</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Life Designer, design by oneself!</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-改善深层神经网络——超参数调试、Batch正则化和程序框架" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/" class="article-date">
  <time datetime="2018-01-14T02:39:49.000Z" itemprop="datePublished">2018-01-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      改善深层神经网络——超参数调试、Batch正则化和程序框架
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="超参数调试、Batch正则化和程序框架"><a href="#超参数调试、Batch正则化和程序框架" class="headerlink" title="超参数调试、Batch正则化和程序框架"></a>超参数调试、Batch正则化和程序框架</h1><h2 id="超参数的调试处理"><a href="#超参数的调试处理" class="headerlink" title="超参数的调试处理"></a>超参数的调试处理</h2><p>在机器学习领域，超参数比较少，我们之前利用设置网格点的方式来调试超参数；<br>但在深度学习领域，超参数较多，不再是设置规则的网格点，而是随机选择点进行调试。这样做是因为，在我们处理问题的时候，无法知道哪个超参数更为重要，随机的方式测试超参数点的性能更为合理，可以探究超参数的潜在价值。<br><a id="more"></a><br><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/1.jpg" alt=""></p>
<h2 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h2><h3 id="Scale-随机均匀取样"><a href="#Scale-随机均匀取样" class="headerlink" title="Scale 随机均匀取样"></a>Scale 随机均匀取样</h3><p>在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有一些超参数的选择做均匀随机取值是不合适的，这里需要按照一定的比例在不同的小范围内进行均匀随机取值，以学习率 $\alpha$ 的选择为例，在 $0.001,\ldots,1$ 范围内进行选择：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/2.jpg" alt=""></p>
<p>如上图所示，如果在 $0.001,\ldots,1$ 的范围内进行进行均匀随机取值，则有90% 的概率 选择范围在 $0.1\sim1$ 之间，而只有 10% 的概率才能选择到$0.001\sim0.1$ 之间，显然是不合理的。</p>
<p>所以在选择的时候，在不同比例范围内进行均匀随机取值，如 $0.001\sim0.001$ 、 $0.001\sim0.01$ 、 $0.01\sim0.1$ 、 $0.1\sim1$ 范围内选择。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">r = <span class="number">-4</span> * np.random.rand()     <span class="comment"># r in [-4,0]</span></div><div class="line">learning_rate = <span class="number">10</span> ** r      <span class="comment"># 10^&#123;r&#125;</span></div></pre></td></tr></table></figure>
<p>在 $10^{a}\sim10^{b}$ 之间的范围内进行随机均匀的选择，则 $r \in [a, b]$ ， $\alpha = 10^{r}$ 。</p>
<p>类似地，在使用<strong>指数加权平均</strong>的时候，超参数 $\beta$ 也需要用这种方式进行随机均匀取样，选择合适的标尺来给超参数取值。</p>
<h2 id="超参数调试实践-Pandas-vs-Caviar"><a href="#超参数调试实践-Pandas-vs-Caviar" class="headerlink" title="超参数调试实践: Pandas vs. Caviar"></a>超参数调试实践: Pandas vs. Caviar</h2><p>在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。下面是不同情况下的两种方式：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/3.jpg" alt=""></p>
<ol>
<li>在计算资源有限的情况下，使用第一种（Pandas 方式），仅调试一个模型，每天不断优化；</li>
<li>在计算资源充足的情况下，使用第二种（Caviar 方式），同时并行调试多个模型，选取其中最好的模型。</li>
</ol>
<h2 id="归一化网络中激活值"><a href="#归一化网络中激活值" class="headerlink" title="归一化网络中激活值"></a>归一化网络中激活值</h2><p>在 Logistic Regression 中，将输入特征进行归一化，可以加速模型的训练。那么对于更深层次的神经网络，我们是否可以归一化隐藏层的输出 $a^{[l]}$ 或者经过激活函数前的 z^{[l]} ，以便加速神经网络的训练过程？答案是肯定的。</p>
<p>常用的方式是：<strong>将隐藏层经过激活函数激活前的 $z^{[l]}$ 进行归一化</strong>。</p>
<h3 id="Batch-Norm-的实现"><a href="#Batch-Norm-的实现" class="headerlink" title="Batch Norm 的实现"></a>Batch Norm 的实现</h3><p>以神经网络中某一隐藏层的中间值为例： $z^{(1)},z^{(2)},\ldots,z^{(m)}$ ：</p>
<p>$$\mu = \dfrac{1}{m}\sum\limits_{i}z^{(i)}$$</p>
<p>$$\sigma^{2}=\dfrac{1}{m}\sum\limits_{i}(z^{(i)}-\mu)^{2}$$</p>
<p>$$z^{(i)}_{\rm norm} = \dfrac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}$$</p>
<p>这里加上 $\varepsilon$ 是为了保证数值的稳定。</p>
<p>到这里所有 $z$ 的分量都是平均值为 0 和方差为 1 的分布，但是我们不希望隐藏层的单元总是如此，也许不同的分布会更有意义，所以我们再进行计算：</p>
<p>$$\widetilde z^{(i)} = \gamma z^{(i)}_{\rm norm}+\beta$$</p>
<p>这里 $\gamma$ 和 $\beta$ 是可以更新学习的参数，如神经网络的权重 $w$ 一样，两个参数的值来确定 $\widetilde z^{(i)}$ 所属的分布。</p>
<p>$\widetilde z^{(i)}$ 来取代 $z^{(i)}$，方便NN中的后续计算。</p>
<h3 id="归一化输入特征X是怎样有助于NN学习"><a href="#归一化输入特征X是怎样有助于NN学习" class="headerlink" title="归一化输入特征X是怎样有助于NN学习"></a>归一化输入特征X是怎样有助于NN学习</h3><p>Batch 归一化的归一化过程不仅适用于输入层，同样适用于NN中的深度隐藏层，不同的是，如果不想使隐藏层的均值为0、方差为1，这个就是为什么有了 $\gamma$ 和 $\beta$ 两个参数后，可以确保所有的$z^(i)$ 可以是你想赋予的任意值，或者说它的作用是保证隐藏单元已使<em>均值</em>和<em>方差**</em>标准化<strong>，均值和方差由这两个参数控制，</strong>使 $z^{(i)}$ 有固定的均值和方差**。</p>
<h2 id="将-Batch-Norm-融入神经网络中"><a href="#将-Batch-Norm-融入神经网络中" class="headerlink" title="将 Batch Norm 融入神经网络中"></a>将 Batch Norm 融入神经网络中</h2><p>在深度神经网络中应用  Batch Norm，这里以一个简单的神经网络为例，前向传播的计算流程如下图所示：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/4.jpg" alt=""></p>
<h3 id="实现梯度下降"><a href="#实现梯度下降" class="headerlink" title="实现梯度下降"></a>实现梯度下降</h3><hr>
<ul>
<li><p>for t = 1 … num （这里num 为Mini Batch 的数量）：</p>
<ul>
<li>在每一个$X^{t}$上进行前向传播（forward prop）的计算：<ul>
<li>在每个隐藏层都用 Batch Norm 将 $z^{[l]}$ 替换为 $\widetilde z^{[l]}$</li>
</ul>
</li>
<li>使用反向传播（Back prop）计算各个参数的梯度： $dw^{[l]}$、$d\gamma^{[l]}$、$d\beta^{[l]}$</li>
<li><p>更新参数：</p>
<ul>
<li>$w^{[l]}:=w^{[l]}-\alpha dw^{[l]}$</li>
<li>$\gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]}$</li>
<li>$\beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]}$</li>
</ul>
</li>
</ul>
</li>
<li><p>与 Mini-batch 梯度下降法一样，Batch Norm 也适用于 Momentum、RMSprop、Adam 的梯度下降法来进行参数更新。</p>
</li>
</ul>
<hr>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><p>这里没有写出偏置参数 $b^{[l]}$ 是因为，对于式子 $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ ，Batch Norm 要做的是将 $z^{[l]}$ 归一化，使其成为均值为0，标准差为1的分布，再由 $\beta$ 和 $\gamma$ 进行重新的分布缩放，那就是意味着：无论 $b^{[l]}$ 值为多少，在这个过程中都会被减去，不会再起作用。所以如果在神经网络中应用 Batch Norm 的话，可以直接将偏置参数 $b^{[l]}$ 去掉，或者将其置为零。</p>
<h2 id="Batch-Norm-起作用的原因"><a href="#Batch-Norm-起作用的原因" class="headerlink" title="Batch Norm 起作用的原因"></a>Batch Norm 起作用的原因</h2><h3 id="First-Reason"><a href="#First-Reason" class="headerlink" title="First Reason"></a>First Reason</h3><p>首先 Batch Norm 可以加速神经网络训练。对输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理是有相同的道理。</p>
<p>Batch Norm 不仅仅将输入的特征进行归一化，而且将各个隐藏层的激活函数的激活值也进行归一化，并调整到另外的分布。</p>
<h3 id="Second-Reason"><a href="#Second-Reason" class="headerlink" title="Second Reason"></a>Second Reason</h3><p>Batch Norm 有效的另外一个原因是它可以使权重比你的网络更滞后或者更深层。</p>
<p>下面是一个判别是否是猫的分类问题，假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果我们将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，那么我们在很大程度上是无法保证能够得到很好的判别结果。</p>
<p>这是因为第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，所以我们无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界。第二个样本集合相当于第一个样本的分布的改变，称为：Covariate shift。如下图所示：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/5.jpg" alt=""></p>
<p>那么存在 Covariate shift 的问题如何应用在神经网络中？就是利用 Batch Norm 来实现。如下面的网络结构：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/6.jpg" alt=""></p>
<p>网络的目的是通过不断的训练，最后输出一个更加接近于真实值的 $\hat y$ 。现在以第2个隐藏层为输入来看：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/7.jpg" alt=""></p>
<p>对于后面的神经网络，是以第二层隐层的输出值 $a^{[2]}$ 作为输入特征的，通过前向传播得到最终的 $\hat y$ ，但是因为我们的网络还有前面两层，由于训练过程，参数 $w^{[1]}$，$w^{[2]}$ 是不断变化的，那么也就是说对于后面的网络， $a^{[2]}$ 的值也是处于不断变化之中，所以就有了Covariate shift的问题。</p>
<p>那么如果对 $z^{[2]}$ 使用了 Batch Norm，那么即使其值不断的变化，但是其均值和方差却会保持。那么 Batch Norm 的作用便是其减少了输入值改变的问题，限制了前层的参数更新导致会影响数值分布的程度，使得网络的后层变得更加稳定。另一个角度就是可以看作，Batch Norm 减弱了前层参数与后层参数之间的联系，使得网络的每层都可以自己进行学习，相对其他层有一定的独立性，这会有助于加速整个网络的学习。</p>
<h3 id="Batch-Norm-正则化效果"><a href="#Batch-Norm-正则化效果" class="headerlink" title="Batch Norm 正则化效果"></a>Batch Norm 正则化效果</h3><p>Batch Norm 还有轻微的正则化效果。</p>
<ul>
<li>这是因为在使用Mini-batch梯度下降的时候，每次计算均值和方差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样就在均值和方差上带来一些比较小的噪声。</li>
<li>因为mini-batch是由一小部分数据估计得来的，在 $z^{[2]}$ 到 $\widetilde z^{[l]}$ 过程也会有一些噪声，因为它是用有些噪声的均值和方差计算得来的。所以和 Dropout 相似，其在每个隐藏层的激活值上加入了一些噪声，（这里因为Dropout 以一定的概率给神经元乘上0或者1）。所以类似于Dropout，Batch Norm 也有轻微的正则化效果，使得下游单元不过分依赖任何一个上游单元。</li>
<li>因为只是引入轻微的正则化效果，所以Batch Norm可以和Dropout一起使用</li>
</ul>
<p>这里有一个小的细节就是，如果使用Batch Norm ，那么使用大的Mini-batch如512，相比使用小的Mini-batch（如64），会引入更少的噪声，那么就会减少正则化的效果。</p>
<h2 id="测试阶段的-Batch-Norm"><a href="#测试阶段的-Batch-Norm" class="headerlink" title="测试阶段的 Batch Norm"></a>测试阶段的 Batch Norm</h2><p>训练过程中，我们是在每个 Mini-batch 使用 Batch Norm，来计算所需要的均值 $\mu$  和方差 $\sigma^{2}$ 。但是在测试的时候，我们需要对每一个测试样本进行预测，无法计算均值和方差。</p>
<p>此时，我们需要单独进行估算均值 $\mu$  和方差 $\sigma^{2}$ 。通常的方法就是在我们训练的过程中，对于训练集的 Mini-batch，使用<strong>指数加权平均</strong>（也叫流动平均 ），当训练结束的时候，得到指数加权平均后的均值  $\mu$  和方差 $\sigma^{2}$ ，而这些值直接用于 Batch Norm 公式的计算，用以对测试样本进行预测。</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/8.jpg" alt=""></p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>在多分类问题中，有一种 logistic regression 的一般形式，叫做 Softmax regression。Softmax 回归可以将多分类任务的输出转换为各个类别可能的概率，从而将最大的概率值所对应的类别作为输入样本的输出类别。</p>
<h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>下图是 Softmax 的公式以及一个简单的例子：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/9.jpg" alt=""></p>
<p>可以看出Softmax通过向量 $z^{[L]}$ 计算出总和为1的四个概率。</p>
<p>在没有隐藏层的时候，直接对 Softmax 层输入样本的特点，则在不同数量的类别下，Sotfmax 层的作用：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/10.jpg" alt=""></p>
<h2 id="训练-Sotfmax-分类器"><a href="#训练-Sotfmax-分类器" class="headerlink" title="训练 Sotfmax 分类器"></a>训练 Sotfmax 分类器</h2><h3 id="理解-Sotfmax"><a href="#理解-Sotfmax" class="headerlink" title="理解 Sotfmax"></a>理解 Sotfmax</h3><p>为什么叫做Softmax？我们以前面的例子为例，由 $z^{[L]}$ 到 $a^{[L]}$ 的计算过程如下：</p>
<p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/11.jpg" alt=""></p>
<p>通常我们判定模型的输出类别，是将输出的最大值对应的类别判定为该模型的类别，也就是说最大值为的位置1，其余位置为0，这也就是所谓的“hard max”。而Sotfmax将模型判定的类别由原来的最大数字5，变为了一个最大的概率0.842，这相对于“hard max”而言，输出更加“soft”而没有那么“hard”。</p>
<p>Sotfmax回归 将 logistic回归 从二分类问题推广到了多分类问题上。</p>
<h3 id="Softmax-的-Loss-function"><a href="#Softmax-的-Loss-function" class="headerlink" title="Softmax 的 Loss function"></a>Softmax 的 Loss function</h3><p>在使用Sotfmax层时，对应的目标值 y 以及训练结束前某次的输出的概率值 $\hat y$ 分别为：</p>
<p>$$y=\left[ \begin{array}{l} 0\1\0\0 \end{array} \right] , \ \hat y=\left[ \begin{array}{l} 0.3\0.2\0.1\0.4 \end{array} \right]$$</p>
<p>Sotfmax使用的 Loss function 为：</p>
<p>$$L(\hat y,y)=-\sum\limits<em>{j=1}^{4}y</em>{j}\log \hat y_{j}$$</p>
<p>在训练过程中，我们的目标是最小化Loss function，由目标值我们可以知道， $y<em>{1}=y</em>{3}=y<em>{4}=0, y</em>{2}=1$ ，所以代入 $L(\hat y,y)$ 中，有：</p>
<p>$$L(\hat y,y)=-\sum\limits<em>{j=1}^{4}y</em>{j}\log \hat y<em>{j}=-y</em>{2}\log \hat y<em>{2}=-\log \hat y</em>{2}$$</p>
<p>所以为了最小化Loss function，我们的目标就变成了使得 $\hat y_{2}$ 的概率尽可能的大。</p>
<p>也就是说，这里的损失函数的作用就是找到你训练集中真实的类别，然后使得该类别相应的概率尽可能地高，这其实是<strong>最大似然估计</strong>的一种形式。</p>
<p>对应的Cost function如下：</p>
<p>$$J(w^{[1]},b^{[1]},\ldots)=\dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat y^{(i)},y^{(i)})$$</p>
<h3 id="Softmax-的梯度下降"><a href="#Softmax-的梯度下降" class="headerlink" title="Softmax 的梯度下降"></a>Softmax 的梯度下降</h3><p>在Softmax层的梯度计算公式为：</p>
<p>$$\dfrac{\partial J}{\partial z^{[L]}}=dz^{[L]} = \hat y -y$$</p>
<h2 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h2><p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/12.jpg" alt=""></p>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><p><img src="/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/13.jpg" alt=""></p>
<p>[1] 吴恩达网络云课堂 deeplearning.ai 课程</p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/30146018" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/30146018</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yespon.github.io/2018/01/14/改善深层神经网络——超参数调试、Batch正则化和程序框架/" data-id="cjetin8zb002oacwgkw3scn42" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>Recommended Posts</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2018/01/15/结构化机器学习项目——机器学习策略-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          结构化机器学习项目——机器学习策略(1)
        
      </div>
    </a>
  
  
    <a href="/2018/01/13/改善深层神经网络——优化算法/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">改善深层神经网络——优化算法</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
           <div id="gitment_comments"></div>
    
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">Content</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#超参数调试、Batch正则化和程序框架"><span class="toc-number">1.</span> <span class="toc-text">超参数调试、Batch正则化和程序框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#超参数的调试处理"><span class="toc-number">1.1.</span> <span class="toc-text">超参数的调试处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为超参数选择合适的范围"><span class="toc-number">1.2.</span> <span class="toc-text">为超参数选择合适的范围</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scale-随机均匀取样"><span class="toc-number">1.2.1.</span> <span class="toc-text">Scale 随机均匀取样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码实现"><span class="toc-number">1.2.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#超参数调试实践-Pandas-vs-Caviar"><span class="toc-number">1.3.</span> <span class="toc-text">超参数调试实践: Pandas vs. Caviar</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#归一化网络中激活值"><span class="toc-number">1.4.</span> <span class="toc-text">归一化网络中激活值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Norm-的实现"><span class="toc-number">1.4.1.</span> <span class="toc-text">Batch Norm 的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#归一化输入特征X是怎样有助于NN学习"><span class="toc-number">1.4.2.</span> <span class="toc-text">归一化输入特征X是怎样有助于NN学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#将-Batch-Norm-融入神经网络中"><span class="toc-number">1.5.</span> <span class="toc-text">将 Batch Norm 融入神经网络中</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实现梯度下降"><span class="toc-number">1.5.1.</span> <span class="toc-text">实现梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Notation"><span class="toc-number">1.5.2.</span> <span class="toc-text">Notation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Norm-起作用的原因"><span class="toc-number">1.6.</span> <span class="toc-text">Batch Norm 起作用的原因</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#First-Reason"><span class="toc-number">1.6.1.</span> <span class="toc-text">First Reason</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Second-Reason"><span class="toc-number">1.6.2.</span> <span class="toc-text">Second Reason</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Norm-正则化效果"><span class="toc-number">1.6.3.</span> <span class="toc-text">Batch Norm 正则化效果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#测试阶段的-Batch-Norm"><span class="toc-number">1.7.</span> <span class="toc-text">测试阶段的 Batch Norm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-回归"><span class="toc-number">1.8.</span> <span class="toc-text">Softmax 回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#计算公式"><span class="toc-number">1.8.1.</span> <span class="toc-text">计算公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练-Sotfmax-分类器"><span class="toc-number">1.9.</span> <span class="toc-text">训练 Sotfmax 分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解-Sotfmax"><span class="toc-number">1.9.1.</span> <span class="toc-text">理解 Sotfmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax-的-Loss-function"><span class="toc-number">1.9.2.</span> <span class="toc-text">Softmax 的 Loss function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax-的梯度下降"><span class="toc-number">1.9.3.</span> <span class="toc-text">Softmax 的梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度学习框架"><span class="toc-number">1.10.</span> <span class="toc-text">深度学习框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensorflow"><span class="toc-number">1.11.</span> <span class="toc-text">Tensorflow</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2018 Yespon&nbsp;|&nbsp;
      Theme by <a href="https://github.com/yespon/hexo-theme-yespon/" target="_blank">Yespon</a>
    </div>
     <div id="footer-right">
      Contact&nbsp;|&nbsp;yespon#qq.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/categories" class="mobile-nav-link">Category</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

  <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
  <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script> 
  <script>
  var gitment = new Gitment({
    // id: '页面 ID', // 可选。默认为 location.href
    owner: 'yespon',
    repo: 'yespon.github.io',
    oauth: {
    client_id: '4ab181ded22ebacbab72',
    client_secret: 'c3cd3df382f34a5685a1608234223423248250f7',
    }
  })
  gitment.render(document.getElementById("gitment_comments"))
</script>


<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>