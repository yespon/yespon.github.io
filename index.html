<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Life Designer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Life Designer, design by oneself!">
<meta property="og:type" content="website">
<meta property="og:title" content="Life Designer">
<meta property="og:url" content="https://yespon.github.io/index.html">
<meta property="og:site_name" content="Life Designer">
<meta property="og:description" content="Life Designer, design by oneself!">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Life Designer">
<meta name="twitter:description" content="Life Designer, design by oneself!">
  
    <link rel="alternate" href="/atom.xml" title="Life Designer" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://yespon.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/categories">Category</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Life Designer</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Life Designer, design by oneself!</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-面试总结" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/29/面试总结/" class="article-date">
  <time datetime="2018-03-29T08:11:49.000Z" itemprop="datePublished">2018-03-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/29/面试总结/">面试总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="算法面试总结"><a href="#算法面试总结" class="headerlink" title="算法面试总结"></a>算法面试总结</h1><h2 id="机器学习部分"><a href="#机器学习部分" class="headerlink" title="机器学习部分"></a>机器学习部分</h2><h3 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h3><h4 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h4><h4 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h4><h4 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h4><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><h4 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h4><h4 id="线性可分与硬间隔"><a href="#线性可分与硬间隔" class="headerlink" title="线性可分与硬间隔"></a>线性可分与硬间隔</h4><h4 id="线性SVM和软间隔"><a href="#线性SVM和软间隔" class="headerlink" title="线性SVM和软间隔"></a>线性SVM和软间隔</h4><h4 id="非线性可分SVM与核函数"><a href="#非线性可分SVM与核函数" class="headerlink" title="非线性可分SVM与核函数"></a>非线性可分SVM与核函数</h4><h4 id="序列最小最优化算法（SMO）"><a href="#序列最小最优化算法（SMO）" class="headerlink" title="序列最小最优化算法（SMO）"></a>序列最小最优化算法（SMO）</h4><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><h4 id="ID3、C4-5、CART"><a href="#ID3、C4-5、CART" class="headerlink" title="ID3、C4.5、CART"></a>ID3、C4.5、CART</h4><h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><h4 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h4><h3 id="集成学习（Ensemble）"><a href="#集成学习（Ensemble）" class="headerlink" title="集成学习（Ensemble）"></a>集成学习（Ensemble）</h3><h4 id="Bagging与随机森林"><a href="#Bagging与随机森林" class="headerlink" title="Bagging与随机森林"></a>Bagging与随机森林</h4><h5 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h5><h5 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h5><h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><h5 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h5><h5 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h5><h5 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h5>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/29/面试总结/" data-id="cjfgjyj43004iicwgdhamhs6z" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-卷积神经网络——目标检测" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/22/卷积神经网络——目标检测/" class="article-date">
  <time datetime="2018-03-22T09:22:38.000Z" itemprop="datePublished">2018-03-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/22/卷积神经网络——目标检测/">卷积神经网络——目标检测</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="1-目标定位和特征点检测"><a href="#1-目标定位和特征点检测" class="headerlink" title="1. 目标定位和特征点检测"></a>1. 目标定位和特征点检测</h2><h3 id="图片检测问题："><a href="#图片检测问题：" class="headerlink" title="图片检测问题："></a>图片检测问题：</h3><ul>
<li>分类问题：判断图中是否为汽车；</li>
<li>目标定位：判断是否为汽车，并确定具体位置；</li>
<li>目标检测：检测不同物体并定位。</li></ul>
        
          <p class="article-more-link">
            <a href="/2018/03/22/卷积神经网络——目标检测/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/22/卷积神经网络——目标检测/" data-id="cjfgjyj1v002cicwgife36upp" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-TF-IDF与余弦相似性" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/16/TF-IDF与余弦相似性/" class="article-date">
  <time datetime="2018-03-16T03:35:21.000Z" itemprop="datePublished">2018-03-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/16/TF-IDF与余弦相似性/">TF-IDF与余弦相似性</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF指Term frequecy,代表词频,IDF代表inverse document frequency,叫做逆文档频率。</p>
<p>这个算法可以用来提取文档的关键词，首先一般认为在文章中出现次数较多的词是关键词，词频就代表了这一项，结果你肯定猜到了，出现次数最多的词是—-“的”、”是”、”在”—-这一类最常用的词。它们叫做”停用词”（stop words），表示对找到结果毫无帮助、必须过滤掉的词。比如过滤之后再统计词频出现了中国，蜜蜂，养殖且三个词的词频几乎一致，但是”中国”是很常见的词，相对而言，”蜜蜂”和”养殖”不那么常见。<br>所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。<br>显然，“中国”这个词出现在其他文章的概率比其他两个词要高不少，因此我们应该认为后两个词更能表现文章的主题，用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。计算该值需要一个语料库，如果一个词在语料库中出现的概率越小，那么该词的IDF应该越大，一般来说TF计算公式为</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/1.png" alt=""></p>
<p>考虑到文章有长短之分，为了便于不同文章的比较，进行”词频”标准化。</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/2.png" alt=""></p>
<p>或者</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/3.png" alt=""></p>
<p>IDF计算公式为：</p>
<p>这里，需要一个语料库（corpus），用来模拟语言的使用环境。</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/4.png" alt=""></p>
<p>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）（平滑）。</p>
<p>将两者乘乘起来就得到了词的TF-IDF。</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/5.png" alt=""></p>
<p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，可以针对不同位置赋予不同的权重进行修正（比如，对全文的第一段和每一段的第一句话，给予较大的权重），注意这些修正之所以是有效的，正是因为人观测过了大量的信息，因此建议了一个先验估计，人将这个先验估计融合到了算法里面，所以使算法更加的有效。</p>
<h2 id="余弦距离是什么，有哪些作用？"><a href="#余弦距离是什么，有哪些作用？" class="headerlink" title="余弦距离是什么，有哪些作用？"></a>余弦距离是什么，有哪些作用？</h2><p>余弦距离是两个向量的距离的一种度量方式，其值在-1~1之间，如果为1表示两个向量同相，0表示两个向量正交，-1表示两个向量反向。两条线段之间形成一个夹角，如果夹角为0度，意味着方向相同、线段重合；如果夹角为90度，意味着形成直角，方向完全不相似；如果夹角为180度，意味着方向正好相反。因此，我们可以<strong>通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。</strong></p>
<h3 id="余弦计算公式"><a href="#余弦计算公式" class="headerlink" title="余弦计算公式"></a>余弦计算公式</h3><p>假定a向量是[x1, y1]，b向量是[x2, y2]，那么可以将余弦定理改写成下面的形式：</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/6.png" alt=""></p>
<p>数学家已经证明，余弦的这种计算方法对n维向量也成立。假定A和B是两个n维向量，A是 [A1, A2, …, An] ，B是 [B1, B2, …, Bn] ，则A与B的夹角θ的余弦等于：</p>
<p><img src="/2018/03/16/TF-IDF与余弦相似性/7.png" alt=""></p>
<p>由此，我们就得到了”找出相似文章”的一种算法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">　　（1）使用TF-IDF算法，找出两篇文章的关键词；</div><div class="line">　　（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；</div><div class="line">　　（3）生成两篇文章各自的词频向量；</div><div class="line">　　（4）计算两个向量的余弦相似度，值越大就表示越相似。</div></pre></td></tr></table></figure>
<p>“余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。</p>
<p>参考：</p>
<ol>
<li><p><a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="external">http://www.ruanyifeng.com/blog/2013/03/tf-idf.html</a></p>
</li>
<li><p>统计学习方法(李航)</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/16/TF-IDF与余弦相似性/" data-id="cjfgjyizn000ricwg24lpejfz" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TF-IDF/">TF-IDF</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/余弦相似度/">余弦相似度</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-ML—模型评估方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/14/ML—模型评估方法/" class="article-date">
  <time datetime="2018-03-14T07:58:12.000Z" itemprop="datePublished">2018-03-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/14/ML—模型评估方法/">ML—模型评估方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="ML—模型评估方法"><a href="#ML—模型评估方法" class="headerlink" title="ML—模型评估方法"></a>ML—模型评估方法</h1><h2 id="1-留出法"><a href="#1-留出法" class="headerlink" title="1. 留出法"></a>1. 留出法</h2><p>“留出法”直接将数据集 D 划分成两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即 $D=S \ T, S \ T = \$。在 S上训练处模型后，用T来评估测试误差，作为对泛化误差的估计。<br>Notation：</p>
<h2 id="2-交叉验证法"><a href="#2-交叉验证法" class="headerlink" title="2. 交叉验证法"></a>2. 交叉验证法</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/14/ML—模型评估方法/" data-id="cjfgjyiyj0002icwgicyswk30" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/模型评估/">模型评估</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-sklearn-preprocessing-数据预处理（OneHotEncoder）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/13/sklearn-preprocessing-数据预处理（OneHotEncoder）/" class="article-date">
  <time datetime="2018-03-13T06:12:57.000Z" itemprop="datePublished">2018-03-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/13/sklearn-preprocessing-数据预处理（OneHotEncoder）/">sklearn preprocessing 数据预处理（OneHotEncoder）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-one-hot编码的由来"><a href="#1-one-hot编码的由来" class="headerlink" title="1. one-hot编码的由来"></a>1. one-hot编码的由来</h2><p>在实际的应用场景中，有非常多的特征不是连续的数值变量，而是某一些离散的类别。比如在广告系统中，用户的性别，用户的地址，用户的兴趣爱好等等一系列特征，都是一些分类值。这些特征一般都无法直接应用在需要进行数值型计算的算法里，比如CTR预估中最常用的LR。那针对这种情况最简单的处理方式是将不同的类别映射为一个整数，比如男性是0号特征，女性为1号特征。这种方式最大的优点就是简单粗暴，实现简单。那最大的问题就是在这种处理方式中，各种类别的特征都被看成是有序的，这显然是非常不符合实际场景的。<br></p>
        
          <p class="article-more-link">
            <a href="/2018/03/13/sklearn-preprocessing-数据预处理（OneHotEncoder）/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/13/sklearn-preprocessing-数据预处理（OneHotEncoder）/" data-id="cjfgjyj1b001ticwgiideecpc" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sklearn/">sklearn</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-TensorFlow基本概念" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/05/TensorFlow基本概念/" class="article-date">
  <time datetime="2018-03-05T07:07:26.000Z" itemprop="datePublished">2018-03-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/05/TensorFlow基本概念/">TensorFlow基本概念</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="TensorFlow基本概念"><a href="#TensorFlow基本概念" class="headerlink" title="TensorFlow基本概念"></a>TensorFlow基本概念</h1><p>开始使用 TensorFlow 时，我们需要了解下其基本组成元素：</p>
<!--nore-->
<ul>
<li><p>使用<strong>计算图（graph )</strong> 定义计算任务</p>
</li>
<li><p>在被称之为<strong>会话 (Session)</strong> 的<strong>上下文 (context)</strong> 中，运行<strong>计算图（graph）</strong></p>
</li>
<li><p>使用<strong>张量（tensor）</strong>表示数据</p>
</li>
<li><p>通过<strong>变量（Variable）</strong>维护状态，对应的还有<strong>常量（constant）</strong></p>
</li>
<li><p>使用<strong>注入（feed）</strong>（喂数据）为任意<strong>操作</strong>(arbitrary <strong>operation</strong>) 赋值，使用<strong>取回（fetch）</strong>从任意操作中获取数据</p>
</li>
</ul>
<p>TensorFlow是一个编程系统，结合下图，我们从基本元素开始说起。</p>
<p><img src="/2018/03/05/TensorFlow基本概念/1.gif" alt=""></p>
<h2 id="1-张量-Tensor"><a href="#1-张量-Tensor" class="headerlink" title="1. 张量(Tensor)"></a>1. 张量(Tensor)</h2><p>名字就是TensorFlow，直观来看，就是张量的流动。张量(tensor)，即任意维度的数据，一维、二维、三维、四维等数据统称为张量。而张量的流动则是指保持计算节点不变，让数据进行流动。这样的设计是针对连接式的机器学习算法，比如逻辑斯底回归，神经网络等。连接式的机器学习算法可以把算法表达成一张图，张量从图中从前到后走一遍就完成了前向运算；而残差从后往前走一遍，就完成了后向传播。</p>
<h3 id="1-1-维度-Shape"><a href="#1-1-维度-Shape" class="headerlink" title="1.1 维度 (Shape)"></a>1.1 维度 (Shape)</h3><p>TensorFlow中使用了三种记号描述张量的维度：阶，形状以及维数。下表展示了他们之间的关系：</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>形状</th>
<th>维数</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[ ]</td>
<td>0-D</td>
<td>一个0维张量是一个常量，如5</td>
</tr>
<tr>
<td>1</td>
<td>[D0]</td>
<td>1-D</td>
<td>一个1维张量是一个矢量，如[5, 4]</td>
</tr>
<tr>
<td>2</td>
<td>[D0, D1]</td>
<td>2-D</td>
<td>一个2维张量是一个矩阵，如[[5, 4], [2, 3]]</td>
</tr>
<tr>
<td>3</td>
<td>[D0, D1, D2]</td>
<td>3-D</td>
<td>一个3维张量是一个立方矩阵，如[[[5, 4], [2, 3]], [[5, 4], [2, 3]]</td>
</tr>
<tr>
<td>n</td>
<td>[D0, D1, D2,….Dn]</td>
<td>n-D</td>
<td>一个n维张量是一个多维数组</td>
</tr>
</tbody>
</table>
<h3 id="1-2-阶（Rank）"><a href="#1-2-阶（Rank）" class="headerlink" title="1.2 阶（Rank）"></a>1.2 阶（Rank）</h3><p>在TensorFlow系统中，张量的维数来被描述为阶。但是张量的阶和矩阵的阶并不是同一个概念：张量的阶（关于如顺序、度数或者是n维）是张量维数的一个数量描述。比如，Python中的list列表就是2阶。</p>
<p>你可以认为零阶张量是一个常量；一阶张量是一个向量；二阶张量就是我们平常所说的矩阵，你可以用语句t[i, j]来访问其中的任何元素；对于三阶张量，你可以用t[i, j, k]来访问其中的任何元素。</p>
<h3 id="1-3-数据类型-Type"><a href="#1-3-数据类型-Type" class="headerlink" title="1.3 数据类型 (Type)"></a>1.3 数据类型 (Type)</h3><p>除了维度，tensor还有一个数据类型属性。你可以为一个张量指定下列数据类型中的任意一个类型：</p>
<p><img src="/2018/03/05/TensorFlow基本概念/2.png" alt=""></p>
<h2 id="2-算子-operation"><a href="#2-算子-operation" class="headerlink" title="2. 算子(operation)"></a>2. 算子(operation)</h2><p>节点被称之为op（节点也叫操作、算子，是operation的缩写）。一个op获得0个或多个tensor，执行计算产生0个或多个tensor。</p>
<h2 id="3-边（edge）"><a href="#3-边（edge）" class="headerlink" title="3. 边（edge）"></a>3. 边（edge）</h2><p>TensorFlow，字面意思就是张量的流动（flow）。<br>TF的图中的边分为两种：</p>
<ul>
<li><p>正常边，正常边上可以流动数据，即正常边就是tensor。计算图的一条边，就是一个tensor。而张量的流动则是指保持计算节点不变，让数据进行流动。tensor是一个数据类型的一维、二维、三维、四维等多维数组。例如，你可以把一组图像集表示为一个四维浮点数的数组，这四个维度分别是 [batch, height, width, channels]。</p>
</li>
<li><p>特殊边，又称作控制依赖，(control dependencies)</p>
<ul>
<li>没有数据从特殊边上流动，但是特殊边却可以控制节点之间的依赖关系，在特殊边的起始节点完成运算之前，特殊边的结束节点不会被执行。</li>
<li>也不仅仅非得有依赖关系才可以用特殊边，还可以有其他用法，比如为了控制内存的时候，可以让两个实际上并没有前后依赖关系的运算分开执行。</li>
<li>特殊边可以在client端被直接使用。</li>
</ul>
</li>
</ul>
<h2 id="4-核-kernel"><a href="#4-核-kernel" class="headerlink" title="4. 核(kernel)"></a>4. 核(kernel)</h2><p>TF中还有一个概念是kernel，kernel是operation在某种设备上的具体实现。TF的库通过注册机制来定义op和kernel，所以可以通过链接一个其他的库来进行kernel和op的扩展。</p>
<h2 id="5-计算图（graph）"><a href="#5-计算图（graph）" class="headerlink" title="5. 计算图（graph）"></a>5. 计算图（graph）</h2><p>节点和边相互连接成计算图，一个计算图描述了一次计算过程。</p>
<p>这是一个声明式的编程方式，如同做菜，我们需要先把主材和佐料都准备好，才能添油烹制。TensorFlow的计算方式也是如此。在构建阶段，我们需要把网络（如神经网络）以计算图的形式构建出来，接着启动会话（session），运行先前构建的图，得到目标结果。</p>
<h2 id="6-会话（session）"><a href="#6-会话（session）" class="headerlink" title="6. 会话（session）"></a>6. 会话（session）</h2><p>使用TensorFlow编写的程序，通常被组织成一个构建阶段和一个运行阶段：在构建阶段，操作的执行步骤被描述成一个计算图；在运行阶段，使用会话执行计算图中的操作。</p>
<p>为了得到结果，计算图必须在会话里被启动。会话将计算图的op分发到诸如CPU或GPU之类的设备上，同时提供执行op的方法。这些方法执行后，将产生的tensor返回。在Python语言中, 返回的tensor是numpy ndarray对象；在C和C++语言中，返回的tensor是tensorflow Tensor实例。</p>
<h2 id="7-变量（variable）"><a href="#7-变量（variable）" class="headerlink" title="7. 变量（variable）"></a>7. 变量（variable）</h2><p>在运行计算图过程中，变量用于维护某个参数的状态。TensorFlow通常会将一个统计模型中的参数表示为一组变量，例如你可以将一个神经网络的权重作为某个变量存储在一个tensor中。在训练过程中, 通过重复运行计算图，更新这个tensor。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="http://blog.csdn.net/weixin_30014549/article/details/52529036" target="_blank" rel="external">tensorflow原理</a></p>
<p>[2] <a href="http://blog.csdn.net/stdcoutzyx/article/details/51645396" target="_blank" rel="external">tensorflow架构</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/05/TensorFlow基本概念/" data-id="cjfgjyizx000xicwgw5eugqou" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-卷积神经网络——卷积神经网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/04/卷积神经网络——卷积神经网络/" class="article-date">
  <time datetime="2018-03-04T04:40:35.000Z" itemprop="datePublished">2018-03-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/04/卷积神经网络——卷积神经网络/">卷积神经网络——卷积神经网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>以下为在吴恩达老师的 deeplearning.ai 课程项目中，第四部分《卷积神经网络》第一周课程 “卷积神经网络基础” 关键点的笔记。本次笔记几乎涵盖了所有视频课程的内容。通过该笔记，一方面为自己学习进行记录，以便以后进行快速review，另一方面，也便于与大家进行探讨学习，错误及不足之处，还望指教。<br><!--nore--></p>
<h2 id="1-计算机视觉"><a href="#1-计算机视觉" class="headerlink" title="1. 计算机视觉"></a>1. 计算机视觉</h2><p>计算机视觉（Computer Vision）包含很多不同类别的问题，如图片分类、目标检测、图片风格迁移等等。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/1.jpg" alt=""></p>
<p>对于小尺寸的图片问题，也许我们用深度神经网络的结构可以较为简单的解决一定的问题。但是当应用在大尺寸的图片上，输入规模将变得十分庞大，使用神经网络将会有非常多的参数需要去学习，这个时候神经网络就不再适用。</p>
<p>卷积神经网络在计算机视觉问题上是一个非常好的网络结构。</p>
<h2 id="2-边缘检测示例"><a href="#2-边缘检测示例" class="headerlink" title="2. 边缘检测示例"></a>2. 边缘检测示例</h2><p>卷积运算是卷积神经网络的基本组成部分。下面以边缘检测的例子来介绍卷积运算。</p>
<p>所谓边缘检测，在下面的图中，分别通过垂直边缘检测和水平边缘检测得到不同的结果：</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/2.jpg" alt=""></p>
<h3 id="垂直边缘检测："><a href="#垂直边缘检测：" class="headerlink" title="垂直边缘检测："></a>垂直边缘检测：</h3><p>假设对于一个 $6\times6$ 大小的图片（以数字表示），以及一个 $3\times3$ 大小的 filter（卷积核） 进行卷积运算，以“ * ” 符号表示。图片和垂直边缘检测器分别如左和中矩阵所示：</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/3.jpg" alt=""></p>
<p>filter 不断地和其大小相同的部分做对应元素的乘法运算并求和，最终得到的数字相当于新图片的一个像素值，如右矩阵所示，最终得到一个 $4\times4$ 大小的图片。</p>
<h3 id="边缘检测的原理："><a href="#边缘检测的原理：" class="headerlink" title="边缘检测的原理："></a>边缘检测的原理：</h3><p>以一个有一条垂直边缘线的简单图片来说明。通过垂直边缘 <strong>filter</strong> 我们得到的最终结果图片可以明显地将边缘和非边缘区分出来：</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/4.jpg" alt=""></p>
<p>卷积运算提供了一个方便的方法来检测图像中的边缘，成为卷积神经网络中重要的一部分。</p>
<h3 id="多种边缘检测："><a href="#多种边缘检测：" class="headerlink" title="多种边缘检测："></a>多种边缘检测：</h3><ul>
<li>垂直和水平边缘检测</li>
</ul>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/5.jpg" alt=""></p>
<ul>
<li>更复杂的 <strong>filter</strong></li>
</ul>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/6.jpg" alt=""></p>
<p>对于复杂的图片，我们可以直接将 <strong>filter</strong> 中的数字直接看作是需要学习的参数，其可以学习到对于图片检测相比上面<strong>filter</strong>更好的更复杂的 <strong>filter</strong> ，如相对于水平和垂直检测器，我们训练的 filter 参数也许可以知道不同角度的边缘。</p>
<p>通过卷积运算，在卷积神经网络中通过反向传播算法，可以学习到相应于目标结果的 <strong>filter</strong>，将其应用于整个图片，输出其提取到的所有有用的特征。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/7.jpg" alt=""></p>
<h3 id="卷积和互相关："><a href="#卷积和互相关：" class="headerlink" title="卷积和互相关："></a>卷积和互相关：</h3><p>在数学定义上，矩阵的卷积（convolution）操作为首先将卷积核同时在水平和垂直方向上进行翻转，构成一个卷积核的镜像，然后使用该镜像再和前面的矩阵进行移动相乘求和操作。如下面例子所示：</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/8.jpg" alt=""></p>
<p>在深度学习中，我们称为的卷积运算实则没有卷积核变换为镜像的这一步操作，因为在权重学习的角度，变换是没有必要的。深度学习的卷积操作在数学上准确度来说称为<strong>互相关</strong>（cross-correlation）。</p>
<h2 id="3-Padding"><a href="#3-Padding" class="headerlink" title="3. Padding"></a>3. Padding</h2><h3 id="没有Padding的缺点："><a href="#没有Padding的缺点：" class="headerlink" title="没有Padding的缺点："></a>没有Padding的缺点：</h3><ul>
<li>每次卷积操作，图片会缩小；</li>
</ul>
<p>就前面的例子来说， $6\times6$ 大小的图片，经过 $3\times3$ 大小的 filter，缩小成了 $4\times4$ 大小</p>
<p>图片： $n\times n–&gt; (n-f+1)\times (n-f+1)$</p>
<ul>
<li>角落和边缘位置的像素进行卷积运算的次数少，可能会丢失有用信息。</li>
</ul>
<p>其中，$\bold n$ 表示图片的长或宽的大小，$\bold{f}$ 表示filter的长或宽的大小。</p>
<h3 id="加Padding："><a href="#加Padding：" class="headerlink" title="加Padding："></a>加Padding：</h3><p>为了解决上面的两个缺点，我们在进行卷积运算前为图片加padding，包围角落和边缘的像素，使得通过filter的卷积运算后，图片大小不变，也不会丢失角落和边沿的信息。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/9.jpg" alt=""></p>
<p>以 p 表示 Padding 的值，则输入 $n\times n$ 大小的图片，最终得到的图片大小为  $(n+2p-f+1)\times (n+2p-f+1)$ ，为使图片大小保持不变，需根据filter的大小调整p的值。</p>
<h3 id="Valid-Same-卷积："><a href="#Valid-Same-卷积：" class="headerlink" title="Valid / Same 卷积："></a>Valid / Same 卷积：</h3><ul>
<li>Valid：no padding；（ $n\times n –&gt; (n-f+1)\times (n-f+1)$ ）</li>
<li>Same：padding，<strong>输出</strong>与<strong>输入</strong>图片大小<strong>相同</strong>，（ $p=(f-1)/2$ ）。在计算机视觉中，一般来说padding的值为奇数（因为filter一般为奇数）</li>
</ul>
<h2 id="4-卷积步长（stride）"><a href="#4-卷积步长（stride）" class="headerlink" title="4. 卷积步长（stride）"></a>4. 卷积步长（stride）</h2><p>卷积的步长是构建卷积神经网络的一个基本的操作。</p>
<p>如前面的例子中，我们使用的 stride=1，每次的卷积运算以1个步长进行移动。下面是 stride=2 时对图片进行卷积的结果：</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/10.jpg" alt=""></p>
<p>以 s 表示 stride 的大小，那么在进行卷积运算后，图片的变化为：</p>
<p>$n\times n –&gt; \left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor\times \left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor$</p>
<p>注意，在当 $padding\ne 1$ 时，若移动的窗口落在图片外面，则不要再进行相乘的操作，丢弃边缘的数值信息，所以输出图片的最终维度为<strong>向下取整</strong>。</p>
<h2 id="5-立体卷积"><a href="#5-立体卷积" class="headerlink" title="5. 立体卷积"></a>5. 立体卷积</h2><h3 id="卷积核的通道数："><a href="#卷积核的通道数：" class="headerlink" title="卷积核的通道数："></a>卷积核的通道数：</h3><p>对于灰色图像中，卷积核和图像均是二维的。而应用于彩色图像中，因为图片有R、G、B三个颜色通道，所以此时的卷积核应为三维卷积核。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/11.jpg" alt=""></p>
<p>卷积核的第三个维度需要与进行卷积运算的图片的通道数相同。</p>
<h3 id="多卷积核："><a href="#多卷积核：" class="headerlink" title="多卷积核："></a>多卷积核：</h3><p>单个卷积核应用于图片时，提取图片特定的特征，不同的卷积核提取不同的特征。如两个大小均为  $3\times3\times3$ 的卷积核分别提取图片的垂直边缘和水平边缘。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/12.jpg" alt=""></p>
<p>由图可知，最终提取到彩色图片的垂直特征图和水平特征图，得到有2个通道的 4\times4 大小的特征图片。</p>
<h3 id="Summary："><a href="#Summary：" class="headerlink" title="Summary："></a>Summary：</h3><p>图片： $(n\times n\times n<em>{c} )* (f\times f\times n</em>{c})$ ——&gt;$(n-f+1)\times (n-f+1)\times n’_{c}$</p>
<p>其中， $n<em>{c}$ 表示通道的数量， $n’</em>{c}$ 表示下一层的通道数，同时也等于本层卷积核的个数。</p>
<h2 id="6-简单卷积网络"><a href="#6-简单卷积网络" class="headerlink" title="6. 简单卷积网络"></a>6. 简单卷积网络</h2><h3 id="单层卷积网络的例子："><a href="#单层卷积网络的例子：" class="headerlink" title="单层卷积网络的例子："></a>单层卷积网络的例子：</h3><p>和普通的神经网络单层前向传播的过程类似，卷积神经网络也是一个先由输入和权重及偏置做线性运算，然后得到的结果输入一个激活函数中，得到最终的输出：</p>
<p>$$z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}$$</p>
<p>$$a^{[1]}=g(z^{[1]})$$</p>
<p>不同点是：在卷积神经网络中，权重和输入进行的是卷积运算。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/13.jpg" alt=""></p>
<h3 id="单层卷积的参数个数："><a href="#单层卷积的参数个数：" class="headerlink" title="单层卷积的参数个数："></a>单层卷积的参数个数：</h3><p>在一个卷积层中，如果我们有10个 $3\times3\times3$ 大小的卷积核，那么加上每个卷积核对应的偏置，则对于一个卷积层，我们共有的参数个数为：</p>
<p>$$(3\times3\times3+1)\times10 = 280$$</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/14.jpg" alt=""></p>
<p>无论图片大小是多少，该例子中的卷积层参数个数一直都是280个，相对于普通的神经网络，卷积神经网络的参数个数要少很多。</p>
<h3 id="标记的总结："><a href="#标记的总结：" class="headerlink" title="标记的总结："></a>标记的总结：</h3><p>如果 $l$ 表示一个卷积层：</p>
<ol>
<li>$f^{[l]}$ ：filter 的大小；</li>
<li>$p^{[l]}$ ：padding；</li>
<li>$s^{[l]}$ ：步长（stride）；</li>
<li>卷积核的个数： $n^{[l]}_{C}$ ；</li>
<li>filter大小： $f^{[l]}\times f^{[l]}\times n^{[l-1]}_{C}$ ;</li>
<li>激活值（Activations）： $a^{[l]}$—&gt;$n^{[l]}<em>{H}\times n^{[l]}</em>{W}\times n^{[l]}_{C}$；</li>
<li>权重（Weights）： $f^{[l]}\times f^{[l]}\times n^{[l-1]}<em>{C}\times n^{[l]}</em>{C}$ ；</li>
<li>偏置（bias）： $n^{[l]}<em>{C}$ — — $(1,1,1,n^{[l]}</em>{C})$</li>
</ol>
<ul>
<li>Input： $n^{[l-1]}<em>{H}\times n^{[l-1]}</em>{W}\times n^{[l-1]}_{C}$ ；</li>
<li>Output： $n^{[l]}<em>{H}\times n^{[l]}</em>{W}\times n^{[l]}_{C}$ ；</li>
</ul>
<p>其中， $n^{[l]}<em>{H} = \left\lfloor \dfrac{n^{[l-1]}</em>{H}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \right\rfloor$ ， $n^{[l]}<em>{W} = \left\lfloor \dfrac{n^{[l-1]}</em>{W}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \right\rfloor$ 。</p>
<h3 id="简单卷积网络示例："><a href="#简单卷积网络示例：" class="headerlink" title="简单卷积网络示例："></a>简单卷积网络示例：</h3><p>多层卷积构成卷积神经网络，下面是一个卷积神经网络的例子：</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/15.jpg" alt=""></p>
<p>卷积网络层的类型：</p>
<ul>
<li>卷积层（Convolution），Conv；</li>
<li>池化层（Pooling），Pool；</li>
<li>全连接层（Fully connected）：Fc；</li>
</ul>
<h2 id="7-池化层"><a href="#7-池化层" class="headerlink" title="7. 池化层"></a>7. 池化层</h2><h3 id="最大池化（Max-pooling）："><a href="#最大池化（Max-pooling）：" class="headerlink" title="最大池化（Max pooling）："></a>最大池化（Max pooling）：</h3><p>最大池化是对前一层得到的特征图进行池化减小，仅由当前小区域内的最大值来代表最终池化后的值。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/16.jpg" alt=""></p>
<p>在最大池化中，有一组超参数需要进行调整，其中， f 表示池化的大小， s 表示步长。</p>
<ul>
<li>池化前： $n \times n$ ；</li>
<li>池化后： $\left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor\times \left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor$ 。</li>
</ul>
<h3 id="平均池化（Average-pooling）："><a href="#平均池化（Average-pooling）：" class="headerlink" title="平均池化（Average pooling）："></a>平均池化（Average pooling）：</h3><p>平均池化与最大池化唯一不同的是其选取的是小区域内的均值来代表该区域内的值。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/17.jpg" alt=""></p>
<h3 id="Pooling-Summary："><a href="#Pooling-Summary：" class="headerlink" title="Pooling Summary："></a>Pooling Summary：</h3><p>池化层的超参数：</p>
<ul>
<li>f ：filter的大小；</li>
<li>s ：stride大小；</li>
<li>最大池化或者平均池化；</li>
<li>p ：padding，这里要注意，几乎很少使用。</li>
</ul>
<p>注意，池化层没有需要学习的参数。</p>
<h2 id="8-卷积神经网络示例"><a href="#8-卷积神经网络示例" class="headerlink" title="8. 卷积神经网络示例"></a>8. 卷积神经网络示例</h2><p>这里以 <strong>LeNet-5</strong> 为例，给出一个完整的卷积神经网络。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/18.jpg" alt=""></p>
<p>构建深度卷积的模式：</p>
<ul>
<li>随着网络的深入，提取的特征图片大小将会逐渐减小，但同时通道数量应随之增加；</li>
<li>Conv——Pool——Conv——Pool——Fc——Fc——Fc——softmax。</li>
</ul>
<h3 id="卷积神经网络的参数："><a href="#卷积神经网络的参数：" class="headerlink" title="卷积神经网络的参数："></a>卷积神经网络的参数：</h3><p><img src="/2018/03/04/卷积神经网络——卷积神经网络/19.jpg" alt=""></p>
<p>根据上表我们可以看出，对于卷积卷积神经网络的参数：</p>
<ul>
<li>在卷积层，仅有少量的参数；</li>
<li>在池化层，没有参数；</li>
<li>在全连接层，存在大量的参数。</li>
</ul>
<h2 id="9-使用卷积神经网络"><a href="#9-使用卷积神经网络" class="headerlink" title="9. 使用卷积神经网络"></a>9. 使用卷积神经网络</h2><h3 id="参数少的优势："><a href="#参数少的优势：" class="headerlink" title="参数少的优势："></a>参数少的优势：</h3><p>与普通的全连接神经网络相比，卷积神经网络的参数更少。如图中的例子，卷积神经网络仅有  6\times(5\times5+1)=156 个参数，而普通的全连接网络有 3072\times4704\approx 14M 个参数。</p>
<p><img src="/2018/03/04/卷积神经网络——卷积神经网络/20.jpg" alt=""></p>
<ul>
<li>参数共享：一个特征检测器（filter）对图片的一部分有用的同时也有可能对图片的另外一部分有用。</li>
<li>连接的稀疏性：在每一层中，每个输出值只取决于少量的输入。</li>
</ul>
<h3 id="训练卷积神经网络："><a href="#训练卷积神经网络：" class="headerlink" title="训练卷积神经网络："></a>训练卷积神经网络：</h3><p><img src="/2018/03/04/卷积神经网络——卷积神经网络/21.jpg" alt=""></p>
<p>我们将训练集输入到卷积神经网络中，对网络进行训练。利用梯度下降（Adam、momentum等优化算法）最小化代价函数来寻找网络的最优参数。</p>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p>[1] deeplearning.ai 课件</p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/30800318" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/30800318</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/03/04/卷积神经网络——卷积神经网络/" data-id="cjfgjyj1v002ficwg6cuwsr9n" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-TensorFlow——Math" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/26/TensorFlow——Math/" class="article-date">
  <time datetime="2018-02-26T07:03:42.000Z" itemprop="datePublished">2018-02-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/26/TensorFlow——Math/">TensorFlow API——Math</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要对tf的一些数学操作方法进行汇总。<br></p>
        
          <p class="article-more-link">
            <a href="/2018/02/26/TensorFlow——Math/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/02/26/TensorFlow——Math/" data-id="cjfgjyizx000ticwg0fr2kwm9" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-Python-yield-及其实现" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/06/Python-yield-及其实现/" class="article-date">
  <time datetime="2018-02-06T12:15:15.000Z" itemprop="datePublished">2018-02-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Python/">Python</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/06/Python-yield-及其实现/">Python yield 及其实现</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Python-yield及其实现"><a href="#Python-yield及其实现" class="headerlink" title="Python yield及其实现"></a>Python yield及其实现</h1><p>刚开始接触python时，解接触到了 yield 关键字，在实际使用中，越来越觉得其用处的强大，遂感觉需整理一下自己的理解，做一个总结。yield 的功能类似于 return，但不同之处在于它返回的是生成器。<br></p>
        
          <p class="article-more-link">
            <a href="/2018/02/06/Python-yield-及其实现/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/02/06/Python-yield-及其实现/" data-id="cjfgjyizn000micwg6yo2hhrs" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/yield-产生器-生成器/">yield 产生器 生成器</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-TensorFlow-api-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/31/TensorFlow-api-1/" class="article-date">
  <time datetime="2018-01-31T15:24:08.000Z" itemprop="datePublished">2018-01-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/31/TensorFlow-api-1/">TensorFlow-api(1)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="TensorFlow-api-1-：tf-reduce-mean-这类函数"><a href="#TensorFlow-api-1-：tf-reduce-mean-这类函数" class="headerlink" title="TensorFlow-api(1)：tf.reduce_mean()这类函数"></a>TensorFlow-api(1)：tf.reduce_mean()这类函数</h1><p>在tensor的某一维度上，有一类求值的函数，如tf.reduce_max( )，tf.reduce_mean( )，tf.reduce_sum( )<br></p>
        
          <p class="article-more-link">
            <a href="/2018/01/31/TensorFlow-api-1/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://yespon.github.io/2018/01/31/TensorFlow-api-1/" data-id="cjfgjyizn000oicwg6ipwwylg" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">Next&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    
  <div class="widget-wrap">
     
        <h3 class="follow-title ">Follow me</h3>
     
    <div class="widget follow">
      
              <a class="github" aria-hidden="true" href="https://github.com/yespon" target="_blank" title="Github"></a>
      
      
            <a class="weibo" aria-hidden="true"  href="http://weibo.com/yespon" target="_blank" title="微博"></a>
      
      
              <a class="zhihu" aria-hidden="true"  href="http://www.zhihu.com/people/yespon" target="_blank" title="知乎"></a>
      
      
            <a class="email" aria-hidden="true"  href="mailto:yespon@qq.com" target="_blank" title="邮箱"></a>
      
    </div>
  </div>


  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title categories">Categories</h3>
    <div class="widget" id="categories">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Framework-Tools/">Framework&Tools</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/服务器技术/">服务器技术</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">30</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构-设计/">架构&设计</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/架构-设计/经典文摘/">经典文摘</a><span class="category-list-count">5</span></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title tagcloud">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CNN-AlexNet/" style="font-size: 14px; color: #00f">CNN AlexNet</a> <a href="/tags/CUDA-GPUs/" style="font-size: 14px; color: #00f">CUDA GPUs</a> <a href="/tags/LDAP/" style="font-size: 14px; color: #00f">LDAP</a> <a href="/tags/MQ/" style="font-size: 19.5px; color: #7741f7">MQ</a> <a href="/tags/NLP/" style="font-size: 14px; color: #00f">NLP</a> <a href="/tags/Nginx/" style="font-size: 14px; color: #00f">Nginx</a> <a href="/tags/Sticky/" style="font-size: 14px; color: #00f">Sticky</a> <a href="/tags/TF-IDF/" style="font-size: 14px; color: #00f">TF-IDF</a> <a href="/tags/TensorFlow/" style="font-size: 16.75px; color: #3c21fb">TensorFlow</a> <a href="/tags/Tensorflow/" style="font-size: 22.25px; color: #b362f2">Tensorflow</a> <a href="/tags/UUID/" style="font-size: 14px; color: #00f">UUID</a> <a href="/tags/decorator/" style="font-size: 14px; color: #00f">decorator</a> <a href="/tags/deeplearning-ai/" style="font-size: 25px; color: #ee82ee">deeplearning.ai</a> <a href="/tags/kNN-算法/" style="font-size: 16.75px; color: #3c21fb">kNN 算法</a> <a href="/tags/pandas-read-csv/" style="font-size: 14px; color: #00f">pandas read_csv</a> <a href="/tags/quarts/" style="font-size: 14px; color: #00f">quarts</a> <a href="/tags/schedule/" style="font-size: 14px; color: #00f">schedule</a> <a href="/tags/sklearn/" style="font-size: 14px; color: #00f">sklearn</a> <a href="/tags/tensorflow/" style="font-size: 14px; color: #00f">tensorflow</a> <a href="/tags/yield-产生器-生成器/" style="font-size: 14px; color: #00f">yield 产生器 生成器</a> <a href="/tags/似然估计-likehood/" style="font-size: 14px; color: #00f">似然估计 likehood</a> <a href="/tags/余弦相似度/" style="font-size: 14px; color: #00f">余弦相似度</a> <a href="/tags/分布式/" style="font-size: 14px; color: #00f">分布式</a> <a href="/tags/反向代理/" style="font-size: 14px; color: #00f">反向代理</a> <a href="/tags/唯一性ID/" style="font-size: 14px; color: #00f">唯一性ID</a> <a href="/tags/性能指标/" style="font-size: 14px; color: #00f">性能指标</a> <a href="/tags/机器学习总结/" style="font-size: 16.75px; color: #3c21fb">机器学习总结</a> <a href="/tags/模型评估/" style="font-size: 14px; color: #00f">模型评估</a> <a href="/tags/消息中间件/" style="font-size: 19.5px; color: #7741f7">消息中间件</a> <a href="/tags/神经网络-NN/" style="font-size: 14px; color: #00f">神经网络 NN</a> <a href="/tags/装饰器/" style="font-size: 14px; color: #00f">装饰器</a> <a href="/tags/集群/" style="font-size: 14px; color: #00f">集群</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/29/面试总结/">面试总结</a>
          </li>
        
          <li>
            <a href="/2018/03/22/卷积神经网络——目标检测/">卷积神经网络——目标检测</a>
          </li>
        
          <li>
            <a href="/2018/03/16/TF-IDF与余弦相似性/">TF-IDF与余弦相似性</a>
          </li>
        
          <li>
            <a href="/2018/03/14/ML—模型评估方法/">ML—模型评估方法</a>
          </li>
        
          <li>
            <a href="/2018/03/13/sklearn-preprocessing-数据预处理（OneHotEncoder）/">sklearn preprocessing 数据预处理（OneHotEncoder）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title archive">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
        <ul>
            
            <li>
                <a href="http://yespon.github.io">yespon&#39;s blog</a>
            </li>
            
        </ul>
    </div>
</div>

  
    <!--微信公众号二维码-->

  <div class="widget-wrap">
    <h3 class="follow-title ">WeChat</h3>
    <div class="widget wechat-widget">
        <img src="/images/wechat_yespon.jpg" alt="扫码关注" width="250"/>
    </div>
  </div>


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2018 Yespon&nbsp;|&nbsp;
      Theme by <a href="https://github.com/yespon/hexo-theme-yespon/" target="_blank">Yespon</a>
    </div>
     <div id="footer-right">
      Contact&nbsp;|&nbsp;yespon#qq.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/categories" class="mobile-nav-link">Category</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

  <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
  <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script> 
  <script>
  var gitment = new Gitment({
    // id: '页面 ID', // 可选。默认为 location.href
    owner: 'yespon',
    repo: 'yespon.github.io',
    oauth: {
    client_id: '4ab181ded22ebacbab72',
    client_secret: 'c3cd3df382f34a5685a1608234223423248250f7',
    }
  })
  gitment.render(document.getElementById("gitment_comments"))
</script>


<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>